{"cells":[{"cell_type":"markdown","id":"51add8c9","metadata":{"id":"51add8c9"},"source":["# Imports"]},{"cell_type":"code","execution_count":183,"id":"59f2afec","metadata":{"executionInfo":{"elapsed":229,"status":"ok","timestamp":1709765844228,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"59f2afec"},"outputs":[],"source":["# import required modules\n","import random\n","from tqdm.notebook import tqdm\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn import model_selection, metrics, preprocessing\n","import copy\n","from torch_geometric.utils import degree\n","\n","import torch\n","from torch import nn, optim, Tensor\n","\n","from torch_sparse import SparseTensor, matmul\n","\n","from torch_geometric.utils import structured_negative_sampling\n","from torch_geometric.data import download_url, extract_zip\n","from torch_geometric.nn.conv.gcn_conv import gcn_norm\n","from torch_geometric.nn.conv import MessagePassing\n","from torch_geometric.typing import Adj"]},{"cell_type":"markdown","id":"a5e5bbd8","metadata":{"id":"a5e5bbd8"},"source":["# Load Dataset"]},{"cell_type":"code","execution_count":184,"id":"Rjdc5Ii_dwMa","metadata":{"executionInfo":{"elapsed":513,"status":"ok","timestamp":1709765883323,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"Rjdc5Ii_dwMa"},"outputs":[],"source":["# Loading CSEN 169 Dataset\n","def get_dataset_table(path: str):\n","    with open(path) as dataset:\n","        T = np.array([[int(elm) for elm in row.split()] for row in dataset])\n","\n","    return T\n","\n","def get_dataset_df(trainpath: str, testpath: str):\n","    users = []\n","    movies = []\n","    ratings = []\n","\n","    T = get_dataset_table(trainpath)\n","    line: np.ndarray\n","    for line in T:\n","        userID, movieID, rating = line.astype(int)\n","\n","        users.append(userID)\n","        movies.append(movieID)\n","        ratings.append(rating)\n","\n","    T = get_dataset_table(testpath)\n","    line: np.ndarray\n","    for line in T:\n","        userID, movieID, rating = line.astype(int)\n","\n","        if rating != 0:\n","            users.append(userID)\n","            movies.append(movieID)\n","            ratings.append(rating)\n","\n","    data = {\n","        'userId': users,\n","        'movieId': movies,\n","        'rating': ratings\n","    }\n","\n","    df = pd.DataFrame(data)\n","    return df"]},{"cell_type":"code","execution_count":185,"id":"5248f04b","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"executionInfo":{"elapsed":1043,"status":"ok","timestamp":1709765932640,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"5248f04b","outputId":"6bfb73b8-faca-45bf-9513-39a9ca59e42f"},"outputs":[{"name":"stdout","output_type":"stream","text":["   userId  movieId  rating\n","0       1        1       5\n","1       1        2       3\n","2       1        4       3\n","3       1        5       3\n","4       1        6       5\n","988\n","200\n"]}],"source":["rating_df = get_dataset_df('data/validation/validation_train.txt', 'data/validation/validation_test.txt')\n","\n","print(rating_df.head())\n","\n","print(len(rating_df['movieId'].unique()))\n","print(len(rating_df['userId'].unique()))\n","\n","rated_movies = set(rating_df['movieId'].unique())\n","\n","rating_df.describe()\n","\n","movieId_to_idx = {}\n","idx_to_movieId = {}\n","idx = 0\n","\n","for i in range(1000):\n","    movieId = i + 1\n","    if movieId not in rated_movies:\n","        movieId_to_idx[movieId] = -1\n","    else:\n","        movieId_to_idx[movieId] = idx\n","        idx_to_movieId[idx] = movieId\n","        idx += 1"]},{"cell_type":"code","execution_count":186,"id":"d3c20490","metadata":{"executionInfo":{"elapsed":260,"status":"ok","timestamp":1709765987953,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"d3c20490"},"outputs":[],"source":["# perform encoding preprocessing to ensure that user_id and item_id are both\n","# in the range of [0, unique_count] so it won't cause out of bound issue when indexing embeddings\n","lbl_user = preprocessing.LabelEncoder()\n","lbl_movie = preprocessing.LabelEncoder()\n","\n","rating_df.userId = lbl_user.fit_transform(rating_df.userId.values)\n","rating_df.movieId = lbl_movie.fit_transform(rating_df.movieId.values)"]},{"cell_type":"code","execution_count":187,"id":"5527b80b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":197,"status":"ok","timestamp":1709765991021,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"5527b80b","outputId":"8523de5b-985f-4195-a121-199a83796990"},"outputs":[{"name":"stdout","output_type":"stream","text":["199\n","987\n"]}],"source":["print(rating_df.userId.max())\n","print(rating_df.movieId.max())"]},{"cell_type":"code","execution_count":188,"id":"ea7018cc","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":232,"status":"ok","timestamp":1709765995096,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"ea7018cc","outputId":"7d8acf07-8350-48fd-e3f0-43fbb3202668"},"outputs":[{"data":{"text/plain":["rating\n","4    4527\n","5    3318\n","3    3236\n","2    1370\n","1     771\n","Name: count, dtype: int64"]},"execution_count":188,"metadata":{},"output_type":"execute_result"}],"source":["rating_df.rating.value_counts()"]},{"cell_type":"code","execution_count":189,"id":"a1e6b7e5","metadata":{"executionInfo":{"elapsed":243,"status":"ok","timestamp":1709765998730,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"a1e6b7e5"},"outputs":[],"source":["# load edges between users and movies\n","def load_edge_csv(df,\n","                  src_index_col,\n","                  dst_index_col,\n","                  link_index_col,\n","                  rating_threshold=3):\n","    \"\"\"Loads csv containing edges between users and items\n","\n","    Args:\n","        src_index_col (str): column name of users\n","        dst_index_col (str): column name of items\n","        link_index_col (str): column name of user item interaction\n","        rating_threshold (int, optional): Threshold to determine positivity of edge. Defaults to 4.\n","\n","    Returns:\n","        list of list: edge_index -- 2 by N matrix containing the node ids of N user-item edges\n","        N here is the number of interactions\n","    \"\"\"\n","\n","    edge_index = None\n","\n","    # Constructing COO format edge_index from input rating events\n","\n","    # get user_ids from rating events in the order of occurance\n","    src = [user_id for user_id in  df['userId']]\n","    # get movie_id from rating events in the order of occurance\n","    dst = [(movie_id) for movie_id in df['movieId']]\n","\n","    # apply rating threshold\n","    edge_attr = torch.from_numpy(df[link_index_col].values).view(-1, 1).to(torch.long) >= rating_threshold\n","\n","    edge_index = [[], []]\n","    for i in range(edge_attr.shape[0]):\n","        if edge_attr[i]:\n","            edge_index[0].append(src[i])\n","            edge_index[1].append(dst[i])\n","    return edge_index"]},{"cell_type":"code","execution_count":190,"id":"49a1fc26","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":352,"status":"ok","timestamp":1709766003613,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"49a1fc26","outputId":"818db39d-ca82-43ac-f776-0edaff5ceedd"},"outputs":[{"name":"stdout","output_type":"stream","text":["2 x 7845\n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 34, 34, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 54, 54, 54, 54, 54, 54, 54, 54, 54, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 60, 60, 60, 60, 60, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 77, 77, 77, 77, 77, 77, 77, 77, 77, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 85, 85, 85, 85, 85, 85, 85, 85, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 97, 97, 97, 97, 97, 97, 97, 97, 97, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 106, 106, 106, 106, 106, 106, 106, 106, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112, 113, 113, 113, 113, 113, 113, 113, 113, 113, 113, 113, 113, 113, 113, 113, 113, 113, 113, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 118, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 121, 121, 121, 121, 121, 121, 121, 121, 121, 121, 121, 121, 121, 121, 121, 121, 121, 121, 121, 121, 121, 121, 121, 121, 121, 121, 121, 121, 121, 121, 121, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 122, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 128, 128, 128, 128, 128, 128, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 132, 132, 132, 132, 132, 132, 132, 132, 133, 133, 133, 133, 133, 133, 133, 133, 133, 133, 133, 133, 134, 134, 134, 134, 134, 134, 134, 134, 134, 134, 134, 134, 134, 134, 134, 134, 134, 134, 134, 134, 134, 134, 134, 134, 134, 134, 134, 135, 135, 135, 135, 135, 135, 135, 135, 135, 135, 135, 135, 135, 135, 135, 135, 135, 135, 135, 135, 135, 135, 135, 135, 135, 135, 135, 135, 135, 135, 136, 136, 136, 136, 136, 136, 136, 136, 136, 136, 136, 136, 136, 136, 136, 136, 136, 136, 136, 136, 136, 136, 136, 136, 136, 136, 136, 136, 136, 136, 136, 136, 136, 136, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 139, 139, 139, 139, 139, 139, 139, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 142, 142, 142, 142, 142, 142, 142, 142, 142, 142, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 145, 145, 145, 145, 145, 145, 145, 145, 145, 145, 145, 145, 145, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 148, 148, 148, 148, 148, 148, 148, 149, 149, 149, 149, 149, 149, 149, 149, 149, 149, 149, 149, 149, 149, 149, 149, 149, 149, 149, 149, 149, 149, 149, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 152, 152, 152, 152, 152, 152, 152, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 153, 154, 154, 155, 155, 155, 155, 155, 155, 155, 155, 155, 155, 155, 155, 155, 155, 155, 155, 155, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 156, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 158, 158, 158, 158, 158, 158, 158, 158, 158, 158, 158, 158, 158, 158, 158, 158, 158, 158, 158, 158, 158, 158, 158, 158, 158, 158, 158, 158, 158, 158, 158, 158, 158, 158, 158, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 159, 160, 161, 161, 161, 162, 162, 162, 162, 163, 163, 163, 163, 163, 164, 164, 164, 164, 165, 166, 166, 166, 167, 167, 167, 168, 168, 168, 168, 168, 169, 169, 170, 170, 170, 170, 171, 171, 171, 172, 172, 172, 172, 173, 173, 173, 174, 174, 174, 174, 174, 175, 175, 175, 175, 176, 176, 176, 176, 177, 177, 177, 177, 178, 178, 178, 178, 179, 179, 180, 180, 181, 181, 181, 182, 183, 183, 184, 184, 184, 184, 185, 185, 185, 186, 186, 186, 186, 186, 187, 187, 187, 187, 188, 188, 188, 189, 189, 190, 190, 190, 191, 191, 191, 191, 191, 192, 192, 192, 193, 193, 193, 194, 194, 194, 195, 195, 195, 196, 197, 197, 197, 198, 198, 198, 198, 199, 199, 199, 199, 199]\n","[0, 5, 8, 11, 13, 15, 19, 21, 22, 24, 31, 32, 38, 43, 44, 45, 46, 47, 49, 50, 55, 56, 59, 60, 63, 64, 65, 67, 71, 74, 75, 76, 78, 79, 80, 81, 83, 85, 86, 88, 89, 90, 92, 95, 97, 99, 105, 106, 107, 108, 112, 113, 114, 118, 120, 122, 126, 127, 128, 131, 132, 133, 134, 136, 143, 145, 149, 150, 153, 155, 156, 159, 160, 161, 162, 165, 167, 168, 169, 170, 172, 173, 174, 175, 176, 177, 180, 181, 182, 183, 184, 185, 186, 189, 190, 191, 192, 193, 194, 195, 196, 198, 201, 202, 205, 207, 208, 209, 211, 213, 220, 221, 222, 223, 226, 227, 228, 229, 233, 234, 235, 237, 238, 240, 241, 245, 247, 248, 249, 250, 252, 255, 256, 257, 264, 266, 267, 268, 269, 0, 12, 13, 24, 49, 99, 110, 126, 236, 241, 250, 254, 256, 268, 272, 275, 276, 278, 281, 282, 283, 284, 291, 292, 294, 296, 298, 299, 300, 302, 305, 309, 310, 312, 315, 180, 259, 317, 319, 320, 326, 327, 328, 330, 341, 343, 347, 10, 49, 257, 259, 270, 287, 293, 299, 302, 323, 326, 328, 353, 356, 359, 0, 16, 23, 28, 39, 41, 49, 61, 69, 88, 94, 99, 100, 108, 120, 134, 152, 168, 171, 172, 182, 185, 188, 193, 203, 207, 208, 210, 221, 226, 227, 234, 256, 266, 379, 382, 387, 388, 393, 398, 405, 419, 420, 425, 427, 429, 430, 432, 433, 440, 443, 0, 8, 11, 13, 18, 22, 31, 49, 55, 58, 63, 70, 80, 86, 88, 97, 99, 123, 126, 130, 131, 132, 133, 134, 135, 136, 152, 164, 165, 167, 168, 169, 172, 173, 174, 176, 177, 179, 182, 184, 185, 186, 190, 191, 193, 194, 196, 198, 207, 208, 210, 212, 220, 222, 237, 241, 268, 271, 273, 274, 301, 303, 317, 356, 405, 407, 416, 424, 429, 432, 458, 459, 460, 463, 464, 466, 471, 472, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 488, 489, 490, 491, 492, 494, 495, 496, 497, 498, 499, 502, 503, 504, 506, 507, 508, 509, 510, 511, 512, 513, 514, 516, 517, 518, 519, 520, 522, 524, 525, 526, 527, 531, 533, 3, 6, 7, 8, 9, 21, 26, 27, 31, 43, 46, 52, 55, 63, 67, 71, 76, 78, 79, 80, 88, 91, 92, 96, 97, 98, 99, 100, 105, 120, 126, 130, 132, 133, 134, 135, 139, 140, 150, 152, 153, 155, 156, 161, 162, 167, 171, 173, 174, 176, 177, 179, 181, 182, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 198, 199, 202, 203, 204, 206, 207, 209, 213, 214, 215, 222, 225, 227, 233, 236, 237, 240, 257, 263, 265, 284, 285, 287, 299, 306, 333, 355, 375, 376, 377, 379, 382, 383, 386, 390, 393, 396, 398, 399, 400, 401, 413, 415, 417, 420, 425, 426, 428, 429, 430, 431, 432, 433, 440, 441, 444, 447, 448, 449, 452, 458, 462, 468, 471, 476, 477, 478, 480, 481, 482, 485, 488, 489, 492, 493, 494, 495, 496, 498, 499, 500, 501, 503, 504, 507, 508, 510, 516, 517, 520, 523, 524, 525, 527, 539, 543, 545, 546, 547, 549, 552, 554, 555, 556, 559, 562, 563, 565, 570, 571, 573, 576, 578, 579, 581, 582, 584, 585, 586, 590, 593, 596, 597, 599, 601, 604, 606, 608, 609, 610, 611, 612, 613, 614, 616, 617, 618, 620, 622, 626, 627, 629, 630, 632, 633, 634, 637, 639, 640, 641, 642, 643, 644, 645, 647, 649, 650, 651, 653, 655, 656, 657, 661, 662, 663, 664, 666, 667, 671, 675, 676, 679, 21, 49, 55, 78, 81, 88, 126, 143, 171, 173, 175, 176, 180, 181, 182, 186, 187, 189, 194, 209, 226, 227, 228, 232, 240, 257, 300, 337, 400, 432, 508, 515, 565, 647, 680, 681, 685, 5, 200, 241, 275, 285, 293, 297, 339, 369, 399, 476, 480, 484, 504, 518, 611, 687, 0, 3, 6, 10, 15, 21, 31, 32, 47, 49, 55, 58, 63, 68, 81, 97, 98, 99, 115, 123, 126, 128, 131, 132, 133, 134, 136, 143, 154, 155, 156, 159, 160, 161, 163, 167, 169, 173, 175, 177, 181, 182, 184, 185, 190, 191, 193, 194, 196, 198, 199, 202, 204, 210, 215, 217, 220, 222, 233, 244, 268, 272, 273, 274, 275, 282, 284, 288, 301, 320, 332, 333, 339, 356, 365, 369, 382, 401, 415, 417, 429, 432, 444, 460, 464, 471, 472, 475, 477, 480, 483, 485, 486, 490, 492, 493, 494, 495, 496, 501, 502, 506, 507, 508, 510, 516, 518, 524, 527, 528, 555, 579, 585, 598, 599, 602, 607, 611, 625, 647, 650, 651, 652, 653, 660, 688, 689, 690, 692, 694, 697, 699, 701, 702, 704, 705, 706, 707, 708, 8, 14, 21, 46, 50, 55, 69, 78, 85, 96, 99, 110, 124, 134, 184, 190, 193, 202, 207, 229, 236, 240, 257, 267, 276, 285, 290, 311, 316, 317, 331, 349, 356, 370, 399, 420, 422, 425, 426, 430, 431, 432, 505, 521, 524, 541, 546, 577, 599, 648, 655, 659, 686, 688, 695, 703, 709, 710, 714, 719, 727, 729, 732, 736, 737, 740, 741, 742, 745, 746, 748, 3, 14, 27, 49, 70, 81, 95, 96, 97, 131, 132, 142, 156, 158, 160, 167, 169, 171, 173, 190, 194, 195, 201, 203, 214, 215, 227, 241, 275, 281, 299, 317, 378, 389, 399, 468, 588, 680, 731, 749, 750, 3, 11, 12, 13, 21, 22, 31, 32, 41, 47, 49, 55, 57, 58, 60, 61, 63, 68, 70, 71, 86, 87, 88, 94, 95, 96, 97, 98, 99, 108, 110, 117, 120, 123, 136, 143, 149, 151, 152, 153, 159, 160, 166, 167, 169, 176, 177, 179, 180, 182, 185, 189, 192, 193, 195, 196, 198, 201, 203, 207, 211, 214, 222, 223, 225, 226, 227, 228, 232, 233, 236, 238, 257, 262, 263, 264, 267, 269, 271, 275, 278, 289, 291, 301, 302, 304, 312, 314, 315, 316, 318, 341, 344, 345, 346, 352, 358, 399, 401, 411, 417, 420, 424, 425, 427, 430, 432, 440, 441, 446, 459, 460, 464, 469, 470, 474, 475, 479, 480, 481, 488, 489, 490, 491, 495, 498, 499, 503, 506, 507, 508, 511, 513, 514, 515, 516, 517, 520, 521, 524, 526, 527, 547, 567, 582, 585, 597, 598, 599, 600, 602, 608, 609, 610, 611, 617, 620, 642, 647, 648, 650, 651, 652, 653, 657, 658, 659, 671, 675, 680, 681, 682, 688, 690, 701, 705, 708, 712, 716, 728, 729, 732, 733, 735, 743, 744, 746, 747, 750, 757, 758, 762, 764, 766, 771, 784, 786, 787, 788, 791, 799, 800, 803, 808, 809, 814, 817, 826, 837, 841, 842, 844, 845, 849, 850, 858, 859, 861, 862, 867, 879, 880, 888, 891, 900, 905, 6, 8, 11, 12, 14, 22, 31, 41, 49, 55, 80, 95, 99, 115, 123, 150, 167, 171, 172, 173, 174, 180, 190, 203, 209, 210, 212, 221, 237, 241, 268, 274, 282, 284, 287, 301, 405, 424, 425, 452, 470, 471, 474, 489, 495, 504, 506, 511, 514, 516, 520, 527, 599, 624, 651, 705, 712, 787, 910, 911, 912, 913, 8, 13, 14, 49, 110, 124, 136, 180, 219, 254, 256, 268, 273, 274, 284, 291, 299, 300, 301, 305, 307, 309, 455, 456, 468, 473, 616, 672, 681, 686, 740, 750, 858, 860, 917, 924, 926, 927, 0, 3, 6, 7, 8, 10, 11, 14, 21, 27, 30, 50, 54, 55, 57, 63, 68, 69, 70, 75, 78, 86, 91, 94, 95, 97, 98, 99, 108, 133, 142, 143, 150, 151, 155, 157, 159, 160, 163, 167, 171, 173, 177, 179, 181, 182, 190, 193, 194, 196, 198, 199, 203, 207, 208, 215, 226, 227, 229, 232, 233, 236, 239, 272, 281, 293, 299, 301, 317, 356, 382, 401, 415, 420, 424, 440, 444, 445, 464, 476, 479, 493, 495, 499, 501, 507, 528, 543, 580, 598, 599, 602, 625, 638, 650, 651, 653, 657, 680, 688, 689, 701, 932, 935, 936, 937, 0, 6, 99, 125, 136, 149, 150, 293, 472, 909, 0, 5, 8, 11, 12, 13, 21, 22, 25, 44, 51, 56, 57, 58, 59, 60, 63, 64, 69, 70, 78, 85, 94, 96, 97, 98, 112, 125, 130, 131, 133, 135, 142, 152, 153, 161, 164, 165, 168, 169, 174, 178, 179, 181, 185, 188, 189, 190, 192, 196, 207, 208, 209, 210, 212, 213, 215, 220, 222, 223, 237, 268, 274, 275, 282, 284, 285, 286, 317, 318, 365, 384, 401, 405, 411, 413, 420, 424, 427, 429, 432, 458, 460, 471, 475, 476, 477, 479, 480, 482, 484, 486, 489, 493, 494, 495, 501, 506, 510, 511, 512, 516, 517, 520, 523, 524, 525, 526, 527, 546, 579, 585, 605, 606, 609, 610, 627, 635, 643, 650, 655, 656, 659, 695, 705, 710, 712, 731, 732, 749, 787, 799, 911, 913, 945, 946, 950, 953, 955, 960, 3, 152, 201, 210, 257, 309, 318, 324, 432, 880, 86, 117, 147, 173, 180, 209, 251, 273, 493, 565, 585, 629, 674, 738, 924, 6, 16, 55, 97, 99, 122, 128, 163, 183, 184, 199, 200, 217, 218, 233, 239, 243, 257, 261, 297, 300, 323, 324, 325, 433, 440, 444, 445, 449, 555, 631, 633, 652, 667, 671, 838, 847, 848, 868, 980, 3, 16, 20, 23, 61, 78, 79, 84, 88, 95, 108, 116, 117, 126, 127, 143, 152, 153, 160, 167, 171, 172, 174, 175, 180, 183, 185, 186, 193, 194, 200, 201, 203, 207, 215, 221, 225, 226, 227, 229, 237, 257, 289, 357, 390, 396, 400, 427, 428, 432, 448, 452, 499, 507, 508, 512, 520, 547, 565, 644, 647, 688, 708, 728, 834, 986, 0, 6, 7, 12, 13, 18, 49, 54, 78, 82, 90, 94, 95, 97, 98, 99, 123, 130, 131, 132, 133, 152, 169, 170, 171, 173, 174, 176, 184, 188, 193, 194, 202, 208, 210, 215, 221, 223, 227, 229, 237, 249, 257, 268, 274, 282, 365, 377, 378, 382, 383, 401, 402, 405, 415, 418, 424, 429, 430, 460, 476, 480, 501, 508, 509, 513, 515, 519, 524, 525, 527, 538, 585, 625, 648, 690, 701, 706, 709, 850, 951, 7, 8, 10, 11, 24, 40, 54, 55, 63, 68, 70, 78, 91, 97, 99, 116, 126, 152, 172, 175, 177, 179, 190, 199, 215, 236, 237, 248, 257, 274, 285, 299, 317, 356, 370, 399, 418, 424, 472, 474, 505, 579, 658, 725, 738, 759, 0, 7, 12, 22, 24, 49, 97, 113, 115, 130, 140, 150, 168, 172, 173, 180, 182, 185, 188, 194, 207, 221, 227, 237, 238, 256, 257, 264, 268, 274, 356, 424, 427, 452, 460, 471, 474, 475, 476, 477, 492, 495, 565, 600, 611, 629, 651, 725, 738, 831, 919, 957, 8, 14, 49, 99, 124, 125, 126, 128, 180, 245, 268, 275, 299, 301, 303, 312, 746, 8, 99, 120, 245, 297, 368, 512, 6, 10, 11, 27, 49, 55, 69, 78, 88, 97, 99, 116, 142, 163, 173, 175, 183, 184, 194, 208, 221, 222, 226, 227, 229, 233, 257, 270, 281, 287, 377, 426, 433, 440, 476, 477, 565, 570, 642, 666, 794, 887, 11, 78, 97, 181, 188, 258, 267, 268, 269, 285, 293, 301, 311, 331, 477, 657, 867, 6, 27, 81, 160, 171, 173, 180, 241, 254, 256, 257, 258, 285, 293, 300, 303, 312, 314, 320, 432, 528, 535, 885, 31, 123, 134, 135, 152, 174, 261, 270, 298, 301, 318, 320, 481, 487, 490, 495, 501, 511, 701, 805, 868, 6, 245, 247, 248, 249, 256, 267, 275, 287, 297, 312, 402, 472, 505, 257, 259, 270, 291, 312, 322, 327, 328, 332, 342, 347, 674, 678, 747, 241, 244, 285, 291, 298, 309, 311, 323, 328, 331, 686, 978, 258, 331, 332, 676, 744, 872, 927, 287, 306, 309, 332, 338, 357, 744, 871, 875, 876, 878, 10, 21, 23, 26, 49, 55, 67, 78, 88, 91, 95, 116, 126, 160, 171, 173, 175, 182, 194, 229, 264, 287, 382, 400, 402, 547, 563, 594, 827, 938, 0, 21, 27, 66, 68, 69, 70, 77, 81, 83, 87, 93, 94, 96, 98, 111, 117, 139, 143, 152, 154, 160, 199, 215, 224, 233, 246, 251, 287, 312, 325, 327, 381, 386, 389, 390, 399, 401, 402, 406, 415, 416, 420, 449, 462, 498, 585, 675, 677, 716, 738, 775, 906, 257, 268, 269, 293, 312, 314, 332, 346, 351, 744, 927, 241, 267, 293, 302, 304, 320, 332, 336, 344, 750, 888, 0, 27, 55, 68, 97, 99, 152, 167, 169, 173, 174, 180, 187, 194, 208, 237, 285, 317, 356, 411, 427, 471, 483, 513, 747, 0, 11, 14, 27, 47, 49, 53, 57, 62, 63, 65, 68, 70, 76, 78, 81, 82, 87, 94, 95, 97, 98, 101, 117, 120, 124, 134, 141, 142, 150, 160, 171, 172, 182, 184, 193, 194, 195, 201, 202, 203, 209, 210, 214, 215, 221, 226, 227, 228, 229, 233, 236, 238, 273, 279, 293, 317, 356, 367, 369, 377, 382, 399, 401, 402, 415, 416, 420, 424, 465, 466, 468, 476, 493, 498, 520, 563, 565, 585, 599, 680, 681, 688, 716, 728, 732, 738, 752, 776, 780, 839, 860, 915, 924, 929, 931, 986, 0, 4, 6, 7, 8, 10, 11, 14, 24, 25, 27, 48, 49, 51, 55, 63, 65, 68, 69, 70, 78, 81, 85, 87, 94, 96, 97, 110, 113, 116, 117, 119, 120, 123, 132, 136, 139, 150, 152, 154, 160, 167, 168, 171, 172, 173, 180, 188, 190, 195, 202, 203, 209, 210, 214, 215, 221, 230, 236, 240, 247, 251, 257, 268, 271, 274, 275, 283, 284, 285, 288, 289, 293, 297, 299, 300, 301, 311, 312, 314, 315, 317, 327, 335, 353, 365, 369, 379, 382, 390, 399, 400, 402, 405, 415, 420, 476, 479, 483, 488, 493, 495, 498, 513, 528, 543, 550, 565, 585, 621, 644, 656, 680, 688, 695, 701, 720, 725, 728, 731, 738, 743, 773, 809, 841, 860, 872, 936, 956, 958, 0, 4, 6, 8, 14, 54, 63, 68, 80, 81, 88, 94, 95, 98, 99, 119, 120, 122, 131, 132, 134, 142, 143, 146, 147, 152, 160, 162, 163, 167, 173, 175, 184, 190, 193, 194, 195, 196, 199, 201, 203, 207, 208, 210, 213, 221, 226, 227, 237, 239, 244, 248, 249, 256, 257, 259, 273, 293, 312, 316, 356, 377, 420, 426, 430, 431, 440, 444, 446, 471, 477, 493, 527, 585, 588, 599, 632, 656, 0, 12, 14, 24, 49, 99, 107, 108, 110, 117, 120, 126, 224, 236, 256, 275, 281, 758, 814, 817, 942, 6, 49, 99, 124, 126, 150, 180, 304, 312, 326, 327, 332, 686, 900, 257, 267, 268, 288, 291, 300, 301, 305, 320, 326, 339, 49, 97, 131, 135, 169, 171, 173, 180, 182, 184, 186, 190, 193, 194, 201, 208, 214, 258, 305, 307, 356, 420, 424, 476, 477, 480, 493, 508, 520, 524, 525, 526, 599, 605, 643, 650, 652, 657, 686, 6, 41, 46, 52, 54, 55, 56, 90, 92, 97, 99, 115, 150, 167, 170, 174, 178, 184, 208, 237, 261, 286, 301, 319, 323, 333, 345, 370, 383, 393, 416, 417, 425, 429, 430, 511, 515, 544, 580, 585, 624, 653, 737, 909, 8, 122, 252, 267, 287, 318, 323, 472, 505, 541, 544, 49, 63, 82, 131, 135, 143, 171, 172, 180, 202, 209, 493, 12, 14, 18, 21, 24, 92, 94, 99, 106, 110, 115, 116, 120, 125, 150, 203, 236, 257, 274, 284, 286, 301, 317, 402, 424, 460, 468, 470, 472, 495, 524, 528, 585, 653, 737, 738, 744, 809, 839, 909, 24, 49, 63, 95, 99, 117, 120, 150, 155, 173, 180, 198, 256, 257, 280, 543, 565, 624, 0, 6, 24, 49, 99, 116, 117, 120, 126, 146, 180, 236, 239, 244, 256, 257, 259, 267, 271, 272, 275, 287, 297, 301, 306, 312, 326, 327, 332, 339, 345, 408, 468, 472, 672, 737, 738, 744, 864, 21, 49, 55, 78, 88, 117, 143, 180, 272, 0, 6, 10, 21, 24, 30, 41, 43, 49, 55, 61, 63, 68, 69, 70, 72, 78, 81, 86, 88, 90, 93, 94, 95, 97, 113, 116, 117, 120, 143, 150, 152, 160, 163, 168, 171, 172, 173, 175, 180, 182, 183, 188, 190, 193, 194, 199, 200, 201, 203, 209, 214, 215, 218, 221, 225, 229, 231, 233, 236, 238, 257, 264, 279, 293, 299, 371, 382, 389, 396, 399, 400, 405, 407, 418, 420, 423, 429, 430, 438, 440, 444, 446, 480, 520, 547, 551, 565, 585, 593, 651, 688, 728, 742, 743, 765, 773, 776, 791, 809, 936, 6, 7, 27, 41, 63, 78, 99, 108, 110, 116, 172, 180, 193, 198, 203, 221, 236, 244, 247, 248, 257, 280, 281, 287, 293, 294, 303, 317, 320, 402, 406, 408, 468, 493, 543, 585, 740, 744, 759, 827, 7, 8, 10, 11, 24, 31, 41, 44, 49, 60, 63, 69, 97, 99, 108, 110, 115, 122, 123, 126, 134, 143, 149, 152, 155, 168, 170, 171, 172, 174, 175, 181, 194, 198, 203, 208, 209, 212, 221, 227, 236, 239, 245, 247, 248, 254, 256, 267, 268, 271, 274, 283, 299, 309, 310, 312, 339, 365, 378, 405, 422, 430, 459, 471, 472, 480, 487, 508, 511, 555, 565, 599, 636, 641, 647, 648, 650, 651, 680, 705, 769, 807, 844, 913, 945, 950, 2, 6, 8, 9, 10, 11, 12, 13, 14, 17, 21, 22, 24, 31, 38, 43, 44, 46, 47, 49, 50, 51, 53, 54, 55, 57, 58, 60, 63, 64, 68, 76, 78, 81, 82, 86, 88, 91, 95, 96, 97, 98, 99, 100, 105, 108, 110, 117, 120, 125, 126, 130, 131, 133, 134, 136, 140, 146, 148, 150, 167, 168, 169, 171, 172, 173, 174, 175, 176, 178, 179, 180, 182, 183, 184, 185, 186, 187, 189, 190, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 207, 208, 209, 210, 211, 214, 215, 217, 225, 227, 229, 233, 237, 240, 264, 275, 276, 286, 287, 312, 317, 320, 322, 356, 365, 369, 378, 379, 382, 399, 400, 418, 420, 422, 424, 425, 426, 427, 428, 429, 430, 432, 433, 440, 444, 445, 448, 455, 459, 463, 471, 476, 477, 480, 481, 486, 487, 488, 493, 495, 500, 501, 503, 505, 507, 508, 510, 511, 513, 518, 520, 524, 525, 526, 546, 547, 556, 559, 563, 565, 566, 567, 578, 579, 580, 581, 588, 602, 606, 611, 612, 614, 618, 636, 638, 640, 643, 645, 646, 647, 650, 651, 653, 656, 659, 660, 666, 668, 669, 671, 675, 695, 698, 701, 704, 705, 709, 711, 720, 725, 731, 732, 735, 737, 742, 743, 751, 758, 760, 766, 776, 784, 787, 817, 819, 839, 840, 849, 891, 909, 918, 953, 961, 964, 6, 8, 11, 12, 22, 27, 29, 46, 49, 55, 58, 59, 60, 63, 68, 69, 72, 76, 78, 87, 88, 94, 95, 97, 120, 130, 131, 133, 135, 143, 150, 151, 159, 160, 161, 162, 165, 167, 171, 172, 173, 175, 177, 178, 179, 180, 182, 184, 185, 193, 194, 196, 198, 199, 203, 204, 207, 208, 209, 210, 211, 214, 215, 217, 221, 226, 228, 229, 233, 264, 271, 274, 285, 326, 356, 364, 375, 382, 390, 402, 413, 414, 417, 424, 426, 427, 430, 431, 432, 440, 442, 471, 476, 477, 479, 481, 482, 486, 487, 488, 489, 490, 491, 495, 499, 502, 503, 504, 505, 507, 508, 510, 511, 512, 514, 520, 521, 522, 525, 526, 543, 555, 579, 590, 598, 600, 602, 604, 609, 611, 613, 629, 633, 634, 637, 646, 652, 655, 656, 657, 661, 667, 669, 671, 680, 695, 701, 704, 725, 731, 732, 741, 751, 793, 804, 829, 836, 257, 299, 303, 327, 346, 3, 7, 8, 11, 12, 13, 19, 23, 46, 49, 54, 55, 58, 64, 68, 70, 80, 81, 82, 88, 90, 95, 97, 99, 113, 120, 124, 126, 131, 134, 152, 161, 163, 167, 170, 172, 173, 175, 178, 179, 180, 181, 182, 194, 195, 198, 208, 209, 212, 215, 221, 234, 237, 249, 257, 272, 282, 297, 305, 317, 356, 377, 400, 418, 430, 460, 461, 470, 471, 472, 480, 495, 505, 509, 518, 524, 579, 647, 648, 656, 660, 693, 695, 708, 712, 850, 868, 939, 945, 949, 12, 24, 49, 99, 108, 115, 149, 223, 249, 250, 254, 276, 285, 299, 300, 332, 402, 405, 472, 505, 744, 807, 0, 6, 7, 8, 10, 11, 21, 30, 49, 55, 63, 68, 69, 71, 80, 86, 90, 94, 95, 97, 99, 110, 126, 131, 134, 140, 142, 153, 155, 156, 159, 167, 172, 174, 175, 178, 180, 181, 182, 184, 185, 186, 187, 189, 190, 193, 194, 195, 198, 201, 202, 208, 210, 214, 215, 221, 227, 228, 229, 234, 236, 237, 264, 268, 283, 287, 309, 312, 317, 339, 365, 378, 382, 400, 420, 422, 426, 428, 431, 433, 444, 472, 508, 512, 513, 517, 524, 565, 579, 585, 629, 632, 658, 680, 701, 732, 742, 773, 949, 14, 24, 27, 47, 49, 63, 72, 86, 87, 96, 97, 120, 124, 134, 167, 184, 190, 195, 196, 201, 209, 210, 214, 215, 238, 293, 317, 327, 355, 375, 390, 399, 420, 424, 426, 432, 508, 511, 523, 528, 647, 651, 656, 657, 672, 731, 732, 773, 800, 946, 49, 126, 180, 236, 247, 248, 257, 279, 280, 287, 293, 297, 468, 505, 737, 738, 759, 6, 23, 24, 63, 104, 120, 122, 124, 150, 239, 275, 402, 469, 739, 827, 8, 24, 49, 116, 177, 236, 257, 274, 285, 287, 472, 6, 8, 11, 41, 47, 49, 55, 97, 99, 116, 122, 123, 149, 150, 171, 173, 180, 181, 196, 233, 235, 245, 255, 257, 264, 267, 287, 288, 297, 301, 320, 505, 879, 7, 23, 27, 47, 49, 68, 81, 82, 87, 88, 94, 95, 98, 127, 131, 134, 142, 151, 167, 168, 172, 173, 175, 180, 182, 184, 185, 188, 192, 196, 201, 207, 209, 216, 221, 227, 229, 263, 264, 297, 299, 312, 390, 396, 400, 401, 405, 416, 420, 448, 479, 480, 493, 498, 504, 508, 524, 585, 747, 13, 55, 63, 64, 88, 134, 174, 196, 256, 275, 345, 356, 426, 459, 472, 511, 740, 913, 4, 8, 11, 14, 24, 27, 44, 47, 50, 55, 63, 68, 69, 76, 78, 86, 95, 96, 97, 99, 116, 123, 126, 128, 133, 134, 146, 160, 173, 176, 179, 181, 186, 190, 193, 194, 195, 203, 209, 211, 225, 232, 233, 240, 264, 317, 355, 356, 379, 399, 420, 424, 432, 463, 468, 473, 476, 477, 481, 490, 501, 506, 512, 517, 518, 522, 523, 524, 525, 527, 547, 550, 563, 565, 578, 579, 588, 599, 624, 640, 651, 680, 681, 860, 961, 6, 11, 31, 55, 58, 63, 80, 88, 99, 126, 134, 153, 155, 170, 172, 174, 178, 179, 186, 195, 212, 268, 271, 285, 317, 356, 379, 430, 471, 472, 476, 477, 515, 653, 656, 6, 8, 12, 14, 99, 120, 236, 257, 271, 275, 293, 301, 306, 312, 325, 327, 332, 339, 686, 0, 12, 24, 55, 78, 99, 107, 110, 113, 120, 136, 150, 189, 195, 234, 270, 272, 289, 300, 402, 405, 407, 408, 424, 457, 469, 472, 474, 493, 505, 681, 692, 942, 5, 6, 22, 55, 58, 59, 60, 63, 69, 88, 91, 92, 97, 99, 134, 136, 149, 171, 174, 191, 196, 199, 215, 275, 285, 323, 471, 510, 511, 514, 528, 805, 0, 22, 27, 49, 51, 55, 88, 152, 153, 167, 172, 173, 174, 175, 178, 194, 200, 208, 237, 245, 267, 428, 471, 480, 481, 495, 515, 520, 637, 92, 236, 254, 256, 287, 288, 300, 409, 873, 0, 5, 6, 18, 49, 99, 115, 123, 136, 245, 250, 257, 261, 267, 268, 282, 285, 302, 305, 318, 324, 339, 579, 686, 736, 807, 891, 897, 44, 57, 85, 86, 99, 204, 207, 214, 236, 302, 463, 480, 528, 879, 0, 2, 6, 24, 41, 97, 120, 146, 168, 185, 236, 272, 274, 275, 279, 281, 282, 317, 407, 472, 588, 592, 722, 918, 0, 7, 8, 10, 13, 63, 68, 69, 70, 72, 96, 98, 99, 110, 120, 132, 167, 168, 173, 174, 177, 180, 190, 193, 196, 198, 201, 210, 215, 264, 275, 283, 285, 293, 309, 356, 365, 411, 415, 427, 429, 452, 459, 476, 477, 478, 479, 493, 501, 510, 511, 515, 516, 520, 526, 585, 653, 657, 0, 1, 14, 21, 27, 30, 37, 42, 62, 63, 65, 68, 69, 76, 81, 93, 94, 96, 105, 109, 116, 120, 124, 160, 173, 180, 185, 190, 195, 203, 209, 214, 215, 232, 233, 251, 254, 264, 273, 280, 297, 322, 337, 355, 382, 390, 402, 406, 420, 462, 465, 524, 543, 563, 565, 572, 573, 577, 581, 588, 605, 619, 656, 659, 680, 681, 688, 713, 716, 718, 728, 735, 751, 764, 776, 778, 856, 858, 922, 6, 11, 14, 30, 63, 86, 94, 97, 110, 116, 120, 147, 150, 193, 221, 224, 236, 244, 257, 264, 272, 273, 275, 285, 288, 299, 382, 463, 474, 483, 520, 525, 526, 540, 740, 744, 860, 872, 7, 9, 13, 22, 26, 27, 49, 55, 56, 57, 63, 68, 69, 70, 82, 85, 86, 88, 94, 97, 98, 123, 126, 131, 132, 133, 134, 135, 142, 151, 153, 167, 169, 173, 174, 178, 179, 181, 186, 189, 190, 191, 193, 196, 198, 202, 204, 207, 208, 210, 212, 214, 233, 245, 257, 267, 271, 285, 297, 300, 312, 317, 318, 344, 356, 370, 375, 377, 379, 390, 411, 416, 417, 420, 422, 429, 432, 440, 446, 448, 459, 461, 462, 475, 476, 477, 478, 480, 482, 485, 489, 493, 495, 496, 499, 501, 503, 504, 506, 507, 508, 510, 511, 513, 514, 520, 523, 524, 525, 528, 579, 600, 602, 627, 637, 638, 643, 650, 653, 655, 656, 657, 660, 701, 703, 704, 705, 787, 807, 913, 945, 241, 257, 268, 269, 326, 679, 881, 882, 1, 3, 6, 7, 8, 21, 24, 26, 37, 47, 49, 54, 61, 62, 63, 66, 69, 78, 79, 81, 86, 87, 88, 93, 95, 96, 99, 110, 117, 126, 131, 134, 143, 152, 153, 160, 162, 166, 171, 176, 178, 180, 181, 182, 185, 187, 194, 195, 198, 201, 203, 207, 208, 209, 210, 215, 227, 228, 229, 232, 238, 253, 273, 280, 317, 365, 381, 382, 390, 402, 407, 408, 424, 432, 448, 469, 488, 493, 499, 507, 511, 512, 516, 520, 532, 547, 551, 563, 565, 574, 582, 624, 625, 647, 653, 680, 688, 701, 718, 724, 728, 775, 776, 778, 785, 790, 796, 839, 860, 864, 916, 934, 260, 285, 300, 307, 310, 314, 353, 686, 874, 879, 890, 895, 0, 6, 13, 48, 49, 82, 87, 99, 110, 116, 120, 126, 149, 150, 180, 186, 196, 212, 215, 221, 234, 235, 236, 239, 245, 256, 267, 268, 274, 276, 282, 320, 378, 384, 472, 514, 690, 698, 703, 720, 728, 807, 809, 873, 926, 5, 7, 8, 9, 10, 11, 13, 16, 19, 21, 24, 25, 29, 30, 32, 41, 51, 55, 56, 58, 63, 64, 69, 78, 82, 85, 88, 95, 96, 97, 99, 126, 130, 131, 132, 133, 135, 140, 142, 152, 153, 154, 161, 165, 169, 173, 176, 177, 178, 179, 184, 186, 189, 190, 192, 193, 196, 197, 198, 202, 210, 211, 212, 215, 217, 220, 222, 233, 236, 240, 241, 268, 269, 271, 284, 285, 286, 300, 301, 302, 305, 306, 310, 311, 312, 316, 317, 321, 339, 346, 355, 356, 365, 379, 382, 384, 399, 418, 420, 422, 424, 432, 440, 444, 459, 461, 468, 471, 475, 477, 479, 480, 482, 483, 485, 486, 487, 490, 491, 493, 494, 496, 498, 503, 504, 506, 509, 512, 516, 518, 520, 523, 524, 525, 526, 528, 565, 598, 599, 600, 602, 603, 605, 606, 607, 609, 610, 613, 614, 627, 628, 635, 640, 643, 644, 646, 647, 648, 650, 653, 655, 656, 657, 658, 686, 688, 695, 701, 703, 705, 709, 728, 735, 805, 807, 830, 831, 849, 857, 891, 894, 896, 913, 935, 944, 948, 954, 955, 956, 960, 961, 982, 21, 27, 30, 49, 63, 68, 81, 96, 97, 126, 133, 134, 135, 142, 171, 173, 175, 180, 181, 186, 191, 194, 203, 204, 209, 229, 233, 263, 264, 288, 312, 317, 321, 326, 327, 330, 337, 350, 356, 420, 424, 426, 432, 476, 477, 480, 481, 492, 504, 508, 512, 517, 523, 526, 597, 599, 608, 610, 612, 647, 653, 685, 731, 746, 0, 3, 4, 6, 7, 8, 10, 11, 30, 41, 45, 46, 47, 49, 50, 55, 57, 63, 70, 78, 88, 91, 92, 95, 97, 99, 116, 120, 123, 124, 128, 133, 134, 143, 152, 153, 155, 156, 158, 159, 163, 167, 170, 171, 173, 174, 175, 178, 179, 180, 181, 182, 185, 188, 189, 190, 192, 194, 195, 197, 202, 203, 207, 208, 209, 213, 214, 217, 218, 221, 222, 233, 236, 237, 244, 247, 249, 251, 257, 264, 267, 272, 273, 275, 281, 290, 303, 306, 312, 379, 400, 405, 408, 418, 422, 425, 428, 433, 460, 463, 468, 471, 472, 497, 505, 512, 515, 518, 525, 528, 563, 578, 579, 588, 615, 624, 636, 647, 651, 656, 659, 669, 670, 675, 688, 711, 723, 727, 733, 743, 773, 784, 817, 819, 937, 945, 951, 953, 980, 0, 14, 221, 234, 274, 282, 473, 474, 809, 839, 0, 3, 6, 7, 8, 10, 11, 21, 22, 23, 27, 30, 31, 41, 44, 48, 49, 51, 52, 53, 54, 55, 57, 60, 63, 67, 69, 75, 78, 80, 81, 82, 91, 92, 96, 97, 99, 108, 110, 131, 132, 134, 142, 152, 153, 155, 156, 159, 167, 169, 172, 173, 174, 175, 176, 178, 179, 180, 182, 184, 185, 186, 189, 190, 191, 192, 193, 195, 199, 200, 202, 203, 205, 208, 209, 210, 213, 214, 216, 218, 222, 227, 233, 234, 240, 245, 249, 256, 257, 264, 267, 272, 273, 285, 301, 312, 316, 317, 342, 345, 346, 355, 370, 378, 383, 387, 396, 398, 399, 401, 407, 417, 418, 422, 425, 428, 429, 430, 432, 433, 440, 444, 445, 448, 455, 461, 464, 466, 467, 468, 472, 480, 481, 498, 501, 503, 505, 506, 507, 515, 522, 524, 525, 534, 546, 578, 585, 586, 599, 612, 621, 625, 638, 640, 642, 643, 646, 647, 648, 650, 651, 653, 675, 680, 681, 682, 688, 689, 692, 711, 723, 731, 732, 737, 740, 746, 784, 787, 800, 807, 818, 911, 913, 932, 939, 949, 951, 958, 980, 0, 6, 13, 14, 21, 27, 30, 47, 50, 63, 64, 67, 68, 69, 70, 76, 78, 82, 87, 90, 93, 95, 96, 97, 98, 101, 110, 116, 120, 126, 138, 140, 141, 142, 143, 150, 152, 167, 169, 171, 172, 173, 174, 177, 180, 182, 185, 189, 190, 194, 195, 196, 197, 198, 201, 203, 206, 207, 208, 209, 215, 225, 227, 231, 232, 256, 273, 281, 285, 327, 355, 356, 364, 375, 378, 382, 386, 390, 401, 413, 416, 417, 420, 433, 442, 460, 468, 471, 482, 488, 492, 493, 506, 507, 512, 515, 517, 520, 524, 529, 536, 547, 565, 588, 618, 621, 623, 645, 646, 647, 651, 653, 656, 679, 688, 701, 732, 738, 743, 837, 957, 0, 6, 7, 22, 49, 55, 63, 78, 86, 88, 90, 95, 97, 99, 126, 143, 152, 155, 169, 173, 175, 180, 181, 182, 184, 186, 189, 194, 197, 215, 237, 264, 317, 442, 471, 476, 480, 481, 516, 641, 669, 0, 6, 22, 27, 31, 49, 78, 81, 88, 95, 96, 97, 114, 131, 134, 152, 167, 168, 173, 174, 182, 188, 190, 194, 201, 203, 221, 227, 356, 405, 425, 426, 427, 429, 431, 432, 479, 599, 651, 657, 659, 666, 909, 24, 46, 115, 425, 432, 511, 514, 520, 625, 0, 3, 6, 10, 11, 21, 49, 55, 63, 68, 78, 91, 99, 116, 124, 146, 167, 172, 180, 181, 195, 202, 203, 209, 231, 236, 237, 239, 257, 272, 287, 289, 299, 312, 314, 327, 337, 345, 361, 365, 367, 399, 400, 402, 407, 468, 472, 505, 543, 588, 592, 594, 615, 624, 647, 672, 738, 744, 747, 759, 775, 823, 257, 268, 271, 293, 299, 301, 312, 315, 327, 343, 346, 354, 686, 687, 746, 748, 872, 891, 23, 49, 116, 120, 124, 146, 180, 236, 254, 256, 283, 287, 402, 738, 914, 49, 88, 94, 97, 100, 174, 182, 185, 194, 201, 203, 207, 227, 257, 306, 318, 507, 585, 608, 23, 49, 95, 116, 125, 126, 143, 180, 249, 254, 300, 468, 484, 524, 14, 49, 99, 125, 149, 180, 256, 268, 269, 271, 275, 282, 284, 288, 289, 301, 312, 315, 344, 472, 588, 624, 746, 747, 257, 267, 268, 269, 271, 285, 287, 301, 312, 323, 326, 0, 7, 11, 13, 21, 24, 27, 63, 68, 76, 96, 106, 161, 164, 190, 193, 195, 209, 210, 212, 215, 222, 243, 274, 284, 285, 312, 492, 523, 563, 579, 581, 656, 680, 695, 699, 773, 913, 257, 267, 268, 301, 304, 311, 339, 893, 6, 9, 13, 49, 99, 123, 126, 136, 274, 289, 293, 318, 512, 714, 0, 6, 10, 11, 14, 16, 30, 57, 66, 68, 69, 70, 71, 76, 78, 81, 87, 88, 90, 93, 94, 95, 97, 99, 110, 116, 120, 124, 146, 155, 156, 158, 163, 171, 172, 173, 175, 176, 178, 180, 182, 190, 194, 195, 201, 203, 209, 210, 217, 221, 222, 226, 228, 229, 232, 233, 236, 238, 251, 257, 264, 287, 293, 294, 317, 355, 363, 371, 377, 385, 390, 399, 400, 402, 408, 420, 446, 448, 505, 517, 528, 547, 563, 565, 585, 623, 632, 661, 731, 735, 738, 751, 803, 839, 860, 10, 11, 21, 27, 32, 40, 53, 63, 68, 76, 78, 81, 87, 93, 95, 160, 187, 195, 232, 257, 271, 287, 306, 312, 314, 325, 332, 399, 418, 420, 448, 563, 678, 680, 688, 785, 241, 257, 268, 300, 301, 312, 314, 327, 332, 339, 353, 244, 271, 285, 288, 299, 301, 302, 305, 306, 309, 311, 315, 321, 330, 338, 345, 685, 746, 747, 750, 872, 880, 927, 49, 99, 125, 126, 245, 254, 256, 257, 267, 272, 285, 293, 302, 322, 324, 325, 326, 327, 505, 592, 867, 965, 88, 97, 99, 155, 174, 175, 178, 182, 194, 196, 268, 356, 471, 479, 480, 493, 642, 655, 3, 6, 7, 8, 10, 11, 12, 22, 31, 32, 47, 49, 55, 81, 88, 91, 123, 126, 136, 171, 173, 175, 176, 177, 180, 182, 186, 227, 233, 239, 272, 281, 316, 356, 428, 440, 459, 463, 472, 476, 508, 527, 638, 650, 758, 841, 942, 969, 10, 179, 180, 186, 190, 192, 194, 198, 202, 220, 249, 257, 258, 267, 270, 284, 288, 309, 312, 343, 345, 387, 476, 481, 508, 512, 516, 651, 657, 726, 746, 800, 891, 0, 10, 11, 14, 24, 32, 49, 55, 95, 97, 108, 116, 120, 143, 149, 150, 155, 167, 171, 172, 173, 175, 178, 180, 194, 209, 213, 221, 236, 256, 257, 264, 267, 297, 306, 312, 357, 402, 418, 420, 594, 624, 674, 738, 747, 759, 768, 784, 879, 6, 21, 22, 31, 52, 54, 55, 78, 97, 131, 133, 134, 155, 163, 170, 171, 173, 174, 175, 179, 183, 184, 192, 199, 200, 209, 217, 222, 257, 287, 319, 323, 393, 410, 418, 430, 433, 471, 472, 508, 510, 525, 544, 548, 555, 556, 599, 637, 650, 651, 668, 671, 770, 794, 838, 847, 909, 950, 8, 10, 21, 23, 24, 39, 49, 53, 55, 82, 85, 86, 88, 95, 99, 108, 110, 116, 120, 124, 131, 136, 143, 146, 153, 167, 173, 180, 181, 187, 192, 198, 208, 209, 212, 221, 234, 236, 244, 258, 267, 271, 273, 274, 276, 281, 285, 287, 299, 300, 309, 312, 321, 327, 331, 339, 353, 379, 382, 389, 402, 409, 448, 452, 456, 469, 483, 503, 508, 534, 543, 547, 559, 565, 588, 594, 624, 654, 680, 681, 685, 693, 706, 712, 714, 737, 738, 758, 807, 820, 823, 839, 872, 907, 971, 982, 0, 8, 14, 24, 120, 124, 257, 281, 285, 402, 738, 914, 8, 13, 56, 82, 85, 97, 99, 123, 126, 134, 136, 155, 164, 180, 191, 196, 236, 274, 293, 312, 356, 424, 425, 476, 505, 506, 512, 627, 640, 713, 738, 927, 45, 69, 82, 134, 174, 179, 186, 189, 190, 192, 211, 214, 268, 375, 384, 400, 420, 461, 466, 506, 507, 508, 579, 657, 704, 711, 720, 723, 732, 733, 946, 8, 21, 22, 97, 99, 126, 134, 142, 164, 181, 191, 196, 241, 274, 275, 284, 285, 318, 429, 432, 459, 476, 479, 480, 482, 501, 508, 511, 653, 703, 841, 6, 10, 95, 97, 143, 153, 165, 167, 194, 225, 547, 612, 0, 7, 21, 27, 49, 63, 68, 71, 72, 78, 79, 82, 86, 87, 89, 93, 97, 115, 133, 135, 142, 143, 157, 162, 171, 172, 173, 175, 180, 189, 190, 193, 194, 201, 203, 208, 209, 238, 257, 269, 274, 282, 289, 293, 299, 317, 365, 390, 398, 424, 427, 431, 432, 448, 452, 475, 476, 480, 482, 490, 493, 495, 508, 510, 517, 565, 582, 644, 655, 701, 728, 747, 752, 785, 935, 986, 242, 257, 261, 265, 287, 299, 301, 310, 312, 314, 327, 336, 339, 343, 352, 677, 747, 871, 877, 61, 221, 226, 227, 228, 242, 257, 270, 287, 293, 299, 342, 377, 446, 447, 744, 892, 0, 13, 14, 25, 47, 49, 63, 64, 70, 78, 81, 82, 85, 87, 97, 98, 117, 120, 130, 132, 135, 139, 142, 158, 160, 167, 172, 180, 181, 189, 190, 195, 196, 203, 208, 209, 215, 236, 237, 264, 273, 275, 282, 293, 299, 316, 318, 339, 365, 375, 390, 402, 414, 415, 419, 420, 422, 424, 430, 448, 462, 468, 475, 479, 480, 484, 487, 491, 496, 502, 505, 565, 585, 588, 598, 599, 605, 618, 647, 680, 682, 688, 711, 728, 732, 735, 759, 785, 832, 932, 939, 945, 956, 241, 268, 271, 285, 301, 897, 0, 1, 2, 4, 10, 11, 21, 23, 26, 27, 30, 32, 37, 38, 41, 43, 48, 49, 53, 54, 55, 61, 62, 63, 64, 65, 66, 67, 68, 70, 78, 81, 83, 88, 89, 93, 94, 97, 98, 104, 110, 117, 124, 127, 131, 133, 142, 146, 147, 149, 157, 158, 160, 171, 173, 175, 178, 180, 182, 183, 184, 187, 194, 195, 199, 201, 203, 209, 214, 215, 217, 221, 225, 227, 228, 233, 234, 235, 236, 239, 245, 248, 251, 254, 260, 266, 267, 268, 269, 270, 275, 280, 281, 285, 287, 288, 293, 297, 304, 306, 312, 314, 315, 320, 321, 327, 328, 331, 332, 342, 345, 353, 355, 356, 357, 364, 365, 372, 376, 382, 389, 390, 400, 402, 407, 408, 409, 415, 417, 420, 423, 424, 440, 446, 449, 452, 462, 466, 469, 474, 493, 498, 505, 524, 528, 529, 531, 535, 543, 547, 551, 552, 561, 563, 565, 575, 585, 586, 593, 594, 615, 621, 623, 638, 654, 665, 668, 674, 678, 680, 688, 725, 735, 738, 742, 744, 752, 759, 761, 768, 774, 794, 802, 810, 821, 827, 869, 874, 883, 886, 887, 919, 924, 929, 934, 936, 949, 963, 964, 980, 0, 8, 13, 18, 99, 123, 125, 126, 241, 250, 268, 275, 284, 285, 301, 312, 740, 746, 839, 11, 55, 123, 126, 136, 153, 250, 284, 481, 518, 520, 912, 257, 268, 271, 307, 314, 315, 745, 746, 0, 14, 257, 312, 315, 322, 325, 327, 536, 744, 747, 872, 11, 22, 54, 55, 76, 97, 172, 175, 182, 184, 202, 227, 233, 257, 293, 320, 324, 326, 440, 467, 472, 501, 599, 638, 649, 740, 929, 8, 13, 14, 18, 55, 88, 99, 115, 116, 123, 126, 136, 203, 222, 236, 268, 274, 275, 282, 285, 297, 302, 317, 472, 512, 522, 643, 740, 743, 841, 14, 49, 54, 88, 116, 117, 120, 143, 171, 173, 180, 182, 194, 209, 221, 234, 236, 242, 248, 249, 256, 260, 265, 299, 326, 382, 402, 408, 469, 543, 594, 676, 683, 744, 11, 12, 14, 25, 44, 55, 97, 99, 110, 116, 120, 132, 136, 150, 181, 184, 186, 193, 208, 210, 221, 237, 284, 317, 356, 432, 480, 481, 490, 493, 506, 510, 511, 514, 515, 516, 520, 598, 599, 610, 613, 658, 99, 126, 149, 245, 267, 285, 287, 295, 296, 302, 306, 455, 505, 672, 267, 285, 288, 301, 303, 318, 873, 6, 14, 24, 49, 99, 105, 116, 117, 119, 120, 124, 125, 146, 221, 236, 243, 249, 251, 254, 257, 273, 280, 281, 283, 285, 290, 294, 297, 299, 312, 321, 322, 327, 332, 406, 407, 468, 469, 532, 543, 594, 615, 672, 674, 692, 713, 740, 809, 819, 916, 920, 963, 974, 6, 27, 81, 123, 168, 175, 180, 185, 188, 332, 345, 349, 405, 422, 511, 887, 271, 306, 312, 314, 324, 325, 327, 330, 332, 346, 0, 3, 7, 8, 11, 13, 14, 18, 19, 21, 23, 31, 32, 47, 49, 54, 63, 64, 65, 68, 69, 86, 95, 97, 115, 116, 123, 124, 126, 128, 134, 143, 152, 164, 171, 172, 173, 175, 179, 180, 186, 189, 190, 192, 193, 194, 195, 196, 212, 214, 215, 233, 236, 241, 247, 250, 256, 257, 272, 281, 284, 285, 292, 296, 302, 303, 312, 315, 317, 325, 356, 390, 402, 420, 432, 458, 468, 471, 475, 477, 497, 505, 511, 518, 520, 521, 525, 528, 530, 585, 594, 628, 643, 647, 650, 651, 695, 705, 709, 725, 738, 743, 746, 747, 773, 780, 839, 841, 849, 873, 891, 932, 946, 953, 6, 11, 12, 21, 22, 30, 38, 41, 43, 49, 53, 55, 65, 68, 78, 87, 88, 99, 105, 108, 116, 122, 133, 134, 149, 155, 158, 163, 171, 172, 173, 175, 180, 181, 182, 184, 194, 199, 201, 202, 208, 215, 218, 221, 226, 229, 236, 237, 241, 245, 249, 256, 257, 259, 264, 269, 272, 277, 280, 281, 283, 293, 298, 300, 301, 312, 314, 315, 326, 327, 328, 341, 342, 345, 347, 351, 353, 355, 357, 361, 390, 407, 409, 428, 444, 445, 467, 468, 507, 512, 546, 549, 563, 566, 569, 588, 594, 599, 627, 631, 632, 646, 648, 661, 669, 670, 680, 681, 684, 686, 709, 728, 746, 747, 748, 757, 759, 784, 810, 819, 872, 915, 929, 939, 244, 257, 261, 271, 301, 312, 314, 318, 330, 335, 339, 344, 345, 257, 268, 291, 300, 303, 304, 312, 318, 338, 339, 344, 686, 746, 890, 895, 0, 6, 7, 49, 55, 68, 69, 70, 88, 113, 115, 131, 134, 150, 162, 167, 168, 171, 174, 180, 208, 213, 221, 226, 227, 356, 405, 429, 470, 471, 492, 498, 504, 506, 526, 585, 593, 659, 980, 267, 268, 285, 301, 304, 345, 888, 0, 12, 49, 92, 126, 128, 146, 150, 180, 220, 234, 245, 267, 272, 275, 287, 292, 318, 323, 407, 455, 472, 624, 0, 3, 6, 8, 9, 11, 13, 14, 24, 32, 43, 49, 50, 51, 55, 57, 63, 64, 65, 68, 69, 78, 80, 82, 85, 86, 87, 88, 92, 96, 97, 110, 113, 120, 124, 130, 131, 132, 133, 135, 136, 150, 153, 159, 161, 162, 163, 167, 168, 169, 170, 171, 174, 177, 180, 184, 185, 188, 192, 193, 195, 196, 197, 203, 207, 208, 209, 210, 212, 215, 221, 222, 223, 226, 227, 233, 237, 264, 273, 274, 285, 286, 299, 316, 317, 320, 356, 369, 370, 375, 378, 379, 384, 405, 408, 411, 417, 422, 425, 426, 427, 429, 432, 440, 448, 458, 459, 460, 461, 463, 470, 471, 475, 477, 479, 480, 481, 482, 483, 484, 485, 486, 487, 491, 493, 494, 496, 501, 502, 503, 504, 506, 509, 511, 513, 519, 520, 522, 525, 526, 546, 579, 598, 599, 601, 602, 605, 610, 624, 628, 629, 634, 648, 650, 651, 653, 655, 656, 657, 658, 659, 695, 699, 703, 705, 720, 725, 728, 731, 732, 766, 777, 787, 807, 829, 831, 839, 841, 909, 929, 943, 946, 955, 958, 960, 961, 7, 14, 21, 48, 50, 65, 66, 68, 70, 79, 96, 110, 116, 120, 124, 131, 132, 142, 150, 152, 154, 156, 160, 161, 166, 172, 190, 203, 214, 219, 233, 236, 240, 254, 271, 273, 274, 277, 279, 283, 285, 312, 362, 369, 399, 407, 408, 420, 480, 484, 501, 524, 546, 628, 681, 688, 695, 712, 716, 720, 735, 736, 771, 775, 776, 778, 780, 785, 788, 860, 956, 55, 63, 78, 181, 257, 264, 356, 49, 60, 88, 134, 173, 174, 181, 190, 196, 210, 237, 285, 301, 356, 411, 471, 476, 477, 479, 481, 485, 512, 524, 636, 647, 704, 800, 909, 285, 320, 47, 57, 85, 136, 177, 179, 186, 191, 196, 210, 316, 317, 356, 507, 637, 647, 657, 0, 49, 99, 116, 126, 136, 146, 149, 267, 268, 272, 273, 275, 282, 285, 288, 289, 292, 297, 312, 339, 404, 407, 505, 512, 0, 7, 9, 10, 19, 23, 37, 38, 49, 54, 55, 61, 78, 81, 84, 88, 91, 95, 99, 110, 115, 117, 120, 123, 126, 128, 136, 143, 147, 153, 162, 167, 171, 172, 173, 174, 175, 176, 181, 186, 187, 189, 193, 201, 203, 208, 216, 227, 237, 240, 243, 249, 270, 274, 276, 282, 283, 285, 292, 324, 365, 400, 405, 411, 427, 432, 452, 468, 480, 499, 508, 513, 522, 527, 559, 565, 573, 577, 590, 632, 648, 655, 682, 690, 738, 740, 741, 766, 792, 804, 819, 974, 6, 23, 24, 95, 110, 116, 125, 126, 219, 224, 242, 244, 248, 257, 258, 272, 275, 293, 309, 322, 332, 402, 448, 468, 473, 543, 592, 674, 681, 752, 823, 860, 864, 908, 920, 0, 3, 12, 22, 23, 31, 49, 55, 60, 78, 92, 99, 116, 122, 123, 134, 136, 149, 150, 159, 167, 168, 173, 174, 181, 184, 186, 191, 194, 200, 201, 208, 210, 217, 233, 239, 247, 272, 275, 281, 284, 292, 407, 427, 444, 452, 458, 459, 460, 471, 472, 480, 481, 494, 505, 511, 528, 541, 599, 600, 667, 689, 759, 766, 912, 945, 13, 0, 10, 24, 55, 63, 96, 97, 99, 116, 117, 120, 124, 14, 90, 126, 168, 257, 7, 82, 95, 0, 14, 24, 49, 126, 132, 133, 171, 244, 291, 257, 261, 267, 268, 123, 176, 182, 241, 259, 267, 268, 8, 11, 13, 8, 10, 11, 30, 49, 6, 12, 49, 92, 6, 10, 11, 21, 0, 1, 6, 7, 257, 271, 287, 299, 55, 67, 6, 8, 49, 68, 110, 54, 0, 8, 8, 22, 24, 27, 30, 37, 43, 7, 22, 27, 63, 64, 4, 6, 12, 21, 0, 3, 9, 14, 116, 85, 271, 285, 6, 24, 49, 99, 107, 0, 22, 24, 0, 3, 8, 13, 46, 54, 7, 24, 66, 21, 0, 6, 10, 6, 8, 13, 92, 0, 1, 6, 7, 8]\n"]}],"source":["edge_index = load_edge_csv(\n","    rating_df,\n","    src_index_col='userId',\n","    dst_index_col='movieId',\n","    link_index_col='rating',\n","    rating_threshold=3.5,\n",")\n","\n","print(f\"{len(edge_index)} x {len(edge_index[0])}\")\n","\n","print(edge_index[0])\n","print(edge_index[1])\n"]},{"cell_type":"code","execution_count":191,"id":"fc37da10","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":359,"status":"ok","timestamp":1709766007118,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"fc37da10","outputId":"3f100807-6f39-4a25-edd4-3416956304d3"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[  0,   0,   0,  ..., 199, 199, 199],\n","        [  0,   5,   8,  ...,   6,   7,   8]])\n","torch.Size([2, 7845])\n","tensor([  0,   0,   0,  ..., 199, 199, 199])\n","tensor([0, 5, 8,  ..., 6, 7, 8])\n"]}],"source":["# Convert to tensor\n","# We use LongTensor here because the .propagate() method in the model needs either LongTensor or SparseTensor\n","edge_index = torch.LongTensor(edge_index)\n","print(edge_index)\n","print(edge_index.size())\n","\n","print(edge_index[0])\n","print(edge_index[1])"]},{"cell_type":"code","execution_count":192,"id":"8c38603a","metadata":{"executionInfo":{"elapsed":186,"status":"ok","timestamp":1709766011338,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"8c38603a"},"outputs":[{"name":"stdout","output_type":"stream","text":["200\n","988\n"]}],"source":["# Note: this is the total num_users and num_movies before we apply the rating_threshold\n","num_users = len(rating_df['userId'].unique())\n","num_movies = len(rating_df['movieId'].unique())\n","\n","print(num_users)\n","print(num_movies)"]},{"cell_type":"code","execution_count":193,"id":"b4e70928","metadata":{"executionInfo":{"elapsed":203,"status":"ok","timestamp":1709766014067,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"b4e70928"},"outputs":[],"source":["num_interactions = edge_index.shape[1]\n","\n","# # split the edges of the graph using a 80/10/10 train/validation/test split\n","# all_indices = [i for i in range(num_interactions)]\n","\n","# train_indices, test_indices = train_test_split(all_indices,\n","#                                                test_size=0.2,\n","#                                                random_state=1)\n","\n","# val_indices, test_indices = train_test_split(test_indices,\n","#                                              test_size=0.5,\n","#                                              random_state=1)\n","\n","# train_edge_index = edge_index[:, train_indices]\n","# val_edge_index = edge_index[:, val_indices]\n","# test_edge_index = edge_index[:, test_indices]"]},{"cell_type":"code","execution_count":194,"id":"ffc7e5e8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":191,"status":"ok","timestamp":1709766016279,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"ffc7e5e8","outputId":"42b48e97-1ea6-44db-d86e-98194d361343"},"outputs":[{"name":"stdout","output_type":"stream","text":["num_users 200, num_movies 988, num_interactions 7845\n","edge_index tensor([[  0,   0,   0,  ..., 199, 199, 199],\n","        [  0,   5,   8,  ...,   6,   7,   8]])\n","1188\n"]}],"source":["print(f\"num_users {num_users}, num_movies {num_movies}, num_interactions {num_interactions}\")\n","print(f\"edge_index {edge_index}\")\n","print((num_users + num_movies))\n","\n","# print(f\"num_users {num_users}, num_movies {num_movies}, num_interactions {num_interactions}\")\n","# print(f\"train_edge_index {train_edge_index}\")\n","# print((num_users + num_movies))\n","# print(torch.unique(train_edge_index[0]).size())\n","# print(torch.unique(train_edge_index[1]).size())"]},{"cell_type":"code","execution_count":195,"id":"e5daf702","metadata":{"executionInfo":{"elapsed":193,"status":"ok","timestamp":1709766018110,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"e5daf702"},"outputs":[],"source":["def convert_r_mat_edge_index_to_adj_mat_edge_index(input_edge_index):\n","    R = torch.zeros((num_users, num_movies))\n","    for i in range(len(input_edge_index[0])):\n","        row_idx = input_edge_index[0][i]\n","        col_idx = input_edge_index[1][i]\n","        R[row_idx][col_idx] = 1\n","\n","    R_transpose = torch.transpose(R, 0, 1)\n","    adj_mat = torch.zeros((num_users + num_movies , num_users + num_movies))\n","    adj_mat[: num_users, num_users :] = R.clone()\n","    adj_mat[num_users :, : num_users] = R_transpose.clone()\n","    adj_mat_coo = adj_mat.to_sparse_coo()\n","    adj_mat_coo = adj_mat_coo.indices()\n","    return adj_mat_coo"]},{"cell_type":"code","execution_count":196,"id":"005195ba","metadata":{"executionInfo":{"elapsed":202,"status":"ok","timestamp":1709766020272,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"005195ba"},"outputs":[],"source":["def convert_adj_mat_edge_index_to_r_mat_edge_index(input_edge_index):\n","    sparse_input_edge_index = SparseTensor(row=input_edge_index[0],\n","                                           col=input_edge_index[1],\n","                                           sparse_sizes=((num_users + num_movies), num_users + num_movies))\n","    adj_mat = sparse_input_edge_index.to_dense()\n","    interact_mat = adj_mat[: num_users, num_users :]\n","    r_mat_edge_index = interact_mat.to_sparse_coo().indices()\n","    return r_mat_edge_index"]},{"cell_type":"code","execution_count":197,"id":"f6e41ca6","metadata":{"executionInfo":{"elapsed":777,"status":"ok","timestamp":1709766022503,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"f6e41ca6"},"outputs":[],"source":["# convert from r_mat (interaction matrix) edge index to adjescency matrix's edge index\n","# so we can feed it to model\n","edge_index = convert_r_mat_edge_index_to_adj_mat_edge_index(edge_index)\n","\n","# # convert from r_mat (interaction matrix) edge index to adjescency matrix's edge index\n","# # so we can feed it to model\n","# train_edge_index = convert_r_mat_edge_index_to_adj_mat_edge_index(train_edge_index)\n","# val_edge_index = convert_r_mat_edge_index_to_adj_mat_edge_index(val_edge_index)\n","# test_edge_index = convert_r_mat_edge_index_to_adj_mat_edge_index(test_edge_index)"]},{"cell_type":"code","execution_count":198,"id":"9ced999a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":184,"status":"ok","timestamp":1709766023635,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"9ced999a","outputId":"17041c50-6542-4008-bf0c-2996416523da"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[   0,    0,    0,  ..., 1186, 1186, 1186],\n","        [ 200,  205,  208,  ...,   21,   41,  124]])\n","torch.Size([2, 15690])\n"]}],"source":["print(edge_index)\n","print(edge_index.size())\n","\n","# print(train_edge_index)\n","# print(train_edge_index.size())\n","# print(val_edge_index)\n","# print(val_edge_index.size())\n","# print(test_edge_index)\n","# print(test_edge_index.size())"]},{"cell_type":"code","execution_count":199,"id":"d8a112c1","metadata":{"executionInfo":{"elapsed":273,"status":"ok","timestamp":1709766025713,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"d8a112c1"},"outputs":[],"source":["# helper function for training and compute BPR loss\n","# since this is a self-supervised learning, we are relying on the graph structure itself and\n","# we don't have label other than the graph structure so we need to the folloing function\n","# which random samples a mini-batch of positive and negative samples\n","def sample_mini_batch(batch_size, edge_index):\n","    \"\"\"Randomly samples indices of a minibatch given an adjacency matrix\n","\n","    Args:\n","        batch_size (int): minibatch size\n","        edge_index (torch.Tensor): 2 by N list of edges\n","\n","    Returns:\n","        tuple: user indices, positive item indices, negative item indices\n","    \"\"\"\n","    # structured_negative_sampling is a pyG library\n","    # Samples a negative edge :obj:`(i,k)` for every positive edge\n","    # :obj:`(i,j)` in the graph given by :attr:`edge_index`, and returns it as a\n","    # tuple of the form :obj:`(i,j,k)`.\n","    #\n","    #         >>> edge_index = torch.as_tensor([[0, 0, 1, 2],\n","    #         ...                               [0, 1, 2, 3]])\n","    #         >>> structured_negative_sampling(edge_index)\n","    #         (tensor([0, 0, 1, 2]), tensor([0, 1, 2, 3]), tensor([2, 3, 0, 2]))\n","    edges = structured_negative_sampling(edge_index)\n","\n","    # 3 x edge_index_len\n","    edges = torch.stack(edges, dim=0)\n","\n","    # here is whhen we actually perform the batch sampe\n","    # Return a k sized list of population elements chosen with replacement.\n","    indices = random.choices([i for i in range(edges[0].shape[0])], k=batch_size)\n","\n","    batch = edges[:, indices]\n","\n","    user_indices, pos_item_indices, neg_item_indices = batch[0], batch[1], batch[2]\n","    return user_indices, pos_item_indices, neg_item_indices"]},{"cell_type":"markdown","id":"3688310d","metadata":{"id":"3688310d"},"source":["# Implementing LightGCN\n","\n","## Light Graph Convolution\n","Between each layer, LightGCN uses the following propagation rule for user and item embeddings.\n","\n","\\begin{equation}\n","e_u^{(k+1)} = \\sum_{i \\in N_u} \\frac{1}{\\sqrt{|N_u|}\\sqrt{|N_i|}} e_i^{(k)} \\quad e_i^{(k+1)} = \\sum_{u \\in N_i} \\frac{1}{\\sqrt{|N_i|}\\sqrt{|N_u|}} e_u^{(k)}\n","\\end{equation}\n","\n","$N_u$: the set of all neighbors of user $u$ (items liked by $u$)\n","\n","$N_i$: the set of all neighbors of item $i$ (users who liked $i$)\n","\n","$e_u^{(k)}$ : k-th layer user embedding\n","\n","$e_i^{(k)}$ : k-th layer item embedding\n","\n","\n","\n","## Layer Combination and Model Prediction\n","The only trainable parameters of LightGCN are the 0-th layer embeddings $e_u^{(0)}$ and $e_i^{(0)}$ for each user and item. We combine the embeddings obtained at each layer of propagation to form the final embeddings for all user and item, $e_u$ and $e_i$ via the follwing equation.\n","\n","\n","\\begin{equation}\n","e_u = \\sum_{k = 0}^K \\alpha_k e_u^{(k)} \\quad e_i = \\sum_{k = 0}^K \\alpha_k e_i^{(k)}\n","\\end{equation}\n","\n","$\\alpha_k$ : hyperparameter which weights the contribution of the k-th layer embedding to the final embedding\n","\n","The model prediction is obtained by taking the inner product of the final user and item embeddings.\n","\n","\\begin{equation}\n","\\hat{y}_{ui} = e_u^Te_i\n","\\end{equation}\n","\n","## Matrix Form\n","In our implementation, we utilize the matrix form of LightGCN. We perform multi-scale diffusion to obtain the final embedding, which sums embeddings diffused across multi-hop scales.\n","\n","\\begin{equation}\n","E^{(K)} = \\alpha_0 E^{(0)} + \\alpha_1 \\tilde{A}^1 E^{(0)} + \\alpha_2 \\tilde{A}^2 E^{(0)} + \\cdot \\cdot \\cdot + \\alpha_K \\tilde{A}^K \\tilde{A} E^{(0)}\n","\\end{equation}\n","\n","$E^{(0)} \\in \\mathcal{R}^{(M + N)} \\times T$ : stacked initial item and user embeddings where $M$, $N$, and $T$ denote the number of users, number of items, and the dimension of each embedding respectively\n","\n","$\\tilde{A} = D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$ : symmetrically normalized adjacency matrix\n"]},{"cell_type":"code","execution_count":200,"id":"9e8cbb67","metadata":{"executionInfo":{"elapsed":198,"status":"ok","timestamp":1709766033422,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"9e8cbb67"},"outputs":[],"source":["# defines LightGCN model\n","class LightGCN(MessagePassing):\n","    \"\"\"LightGCN Model as proposed in https://arxiv.org/abs/2002.02126\n","    \"\"\"\n","\n","    def __init__(self, num_users,\n","                 num_items,\n","                 embedding_dim=64, # define the embding vector length for each node\n","                 K=3,\n","                 add_self_loops=False):\n","        \"\"\"Initializes LightGCN Model\n","\n","        Args:\n","            num_users (int): Number of users\n","            num_items (int): Number of items\n","            embedding_dim (int, optional): Dimensionality of embeddings. Defaults to 8.\n","            K (int, optional): Number of message passing layers. Defaults to 3.\n","            add_self_loops (bool, optional): Whether to add self loops for message passing. Defaults to False.\n","        \"\"\"\n","        super().__init__()\n","        self.num_users = num_users\n","        self.num_items = num_items\n","        self.embedding_dim = embedding_dim\n","        self.K = K\n","        self.add_self_loops = add_self_loops\n","\n","        # define user and item embedding for direct look up.\n","        # embedding dimension: num_user/num_item x embedding_dim\n","\n","        self.users_emb = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.embedding_dim) # e_u^0\n","\n","        self.items_emb = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.embedding_dim) # e_i^0\n","\n","        # \"Fills the input Tensor with values drawn from the normal distribution\"\n","        # according to LightGCN paper, this gives better performance\n","        nn.init.normal_(self.users_emb.weight, std=0.1)\n","        nn.init.normal_(self.items_emb.weight, std=0.1)\n","\n","    def forward(self, edge_index: Tensor):\n","        \"\"\"Forward propagation of LightGCN Model.\n","\n","        Args:\n","            edge_index (SparseTensor): adjacency matrix\n","\n","        Returns:\n","            tuple (Tensor): e_u_k, e_u_0, e_i_k, e_i_0\n","        \"\"\"\n","\n","\n","        \"\"\"\n","            compute \\tilde{A}: symmetrically normalized adjacency matrix\n","            \\tilde_A = D^(-1/2) * A * D^(-1/2)    according to LightGCN paper\n","\n","            this is essentially a metrix operation way to get 1/ (sqrt(n_neighbors_i) * sqrt(n_neighbors_j))\n","\n","\n","            if your original edge_index look like\n","            tensor([[   0,    0,    0,  ...,  609,  609,  609],\n","                    [   0,    2,    5,  ..., 9444, 9445, 9485]])\n","\n","                    torch.Size([2, 99466])\n","\n","            then this will output:\n","                (\n","                 tensor([[   0,    0,    0,  ...,  609,  609,  609],\n","                         [   0,    2,    5,  ..., 9444, 9445, 9485]]),\n","                 tensor([0.0047, 0.0096, 0.0068,  ..., 0.0592, 0.0459, 0.1325])\n","                 )\n","\n","              where edge_index_norm[0] is just the original edge_index\n","\n","              and edge_index_norm[1] is the symmetrically normalization term.\n","\n","            under the hood it's basically doing\n","                def compute_gcn_norm(edge_index, emb):\n","                    emb = emb.weight\n","                    from_, to_ = edge_index\n","                    deg = degree(to_, emb.size(0), dtype=emb.dtype)\n","                    deg_inv_sqrt = deg.pow(-0.5)\n","                    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n","                    norm = deg_inv_sqrt[from_] * deg_inv_sqrt[to_]\n","\n","                    return norm\n","\n","\n","        \"\"\"\n","        edge_index_norm = gcn_norm(edge_index=edge_index,\n","                                   add_self_loops=self.add_self_loops)\n","\n","        # concat the user_emb and item_emb as the layer0 embing matrix\n","        # size will be (n_users + n_items) x emb_vector_len.   e.g: 10334 x 64\n","        emb_0 = torch.cat([self.users_emb.weight, self.items_emb.weight]) # E^0\n","\n","        embs = [emb_0] # save the layer0 emb to the embs list\n","\n","        # emb_k is the emb that we are actually going to push it through the graph layers\n","        # as described in lightGCN paper formula 7\n","        emb_k = emb_0\n","\n","        # push the embedding of all users and items through the Graph Model K times.\n","        # K here is the number of layers\n","        for i in range(self.K):\n","            emb_k = self.propagate(edge_index=edge_index_norm[0], x=emb_k, norm=edge_index_norm[1])\n","            embs.append(emb_k)\n","\n","\n","        # this is doing the formula8 in LightGCN paper\n","\n","        # the stacked embs is a list of embedding matrix at each layer\n","        #    it's of shape n_nodes x (n_layers + 1) x emb_vector_len.\n","        #        e.g: torch.Size([10334, 4, 64])\n","        embs = torch.stack(embs, dim=1)\n","\n","        # From LightGCn paper: \"In our experiments, we find that setting α_k uniformly as 1/(K + 1)\n","        #    leads to good performance in general.\"\n","        emb_final = torch.mean(embs, dim=1) # E^K\n","\n","\n","        # splits into e_u^K and e_i^K\n","        users_emb_final, items_emb_final = torch.split(emb_final, [self.num_users, self.num_items])\n","\n","        # returns e_u^K, e_u^0, e_i^K, e_i^0\n","        # here using .weight to get the tensor weights from n.Embedding\n","        return users_emb_final, self.users_emb.weight, items_emb_final, self.items_emb.weight\n","\n","    def message(self, x_j, norm):\n","        # x_j is of shape:  edge_index_len x emb_vector_len\n","        #    e.g: torch.Size([77728, 64]\n","        #\n","        # x_j is basically the embedding of all the neighbors based on the src_list in coo edge index\n","        #\n","        # elementwise multiply by the symmetrically norm. So it's essentiall what formula 7 in LightGCN\n","        # paper does but here we are using edge_index rather than Adj Matrix\n","        return norm.view(-1, 1) * x_j\n","\n","layers = 3\n","model = LightGCN(num_users=num_users,\n","                 num_items=num_movies,\n","                 K=layers)"]},{"cell_type":"code","execution_count":null,"id":"ea1884a0","metadata":{"id":"ea1884a0"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"ffad4fa8","metadata":{"id":"ffad4fa8"},"source":["# Loss Function\n","\n","\n","\n","We utilize a Bayesian Personalized Ranking (BPR) loss, a pairwise objective which encourages the predictions of positive samples to be higher than negative samples for each user.\n","\n","\\begin{equation}\n","L_{BPR} = -\\sum_{u = 1}^M \\sum_{i \\in N_u} \\sum_{j \\notin N_u} \\ln{\\sigma(\\hat{y}_{ui} - \\hat{y}_{uj})} + \\lambda ||E^{(0)}||^2\n","\\end{equation}\n","\n","$\\hat{y}_{u}$: predicted score of a positive sample\n","\n","$\\hat{y}_{uj}$: predicted score of a negative sample\n","\n","$\\lambda$: hyperparameter which controls the L2 regularization strength"]},{"cell_type":"code","execution_count":201,"id":"cee24091","metadata":{"executionInfo":{"elapsed":211,"status":"ok","timestamp":1709766042996,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"cee24091"},"outputs":[],"source":["def bpr_loss(users_emb_final,\n","             users_emb_0,\n","             pos_items_emb_final,\n","             pos_items_emb_0,\n","             neg_items_emb_final,\n","             neg_items_emb_0,\n","             lambda_val):\n","    \"\"\"Bayesian Personalized Ranking Loss as described in https://arxiv.org/abs/1205.2618\n","\n","    Args:\n","        users_emb_final (torch.Tensor): e_u_k\n","        users_emb_0 (torch.Tensor): e_u_0\n","        pos_items_emb_final (torch.Tensor): positive e_i_k\n","        pos_items_emb_0 (torch.Tensor): positive e_i_0\n","        neg_items_emb_final (torch.Tensor): negative e_i_k\n","        neg_items_emb_0 (torch.Tensor): negative e_i_0\n","        lambda_val (float): lambda value for regularization loss term\n","\n","    Returns:\n","        torch.Tensor: scalar bpr loss value\n","    \"\"\"\n","    reg_loss = lambda_val * (users_emb_0.norm(2).pow(2) +\n","                             pos_items_emb_0.norm(2).pow(2) +\n","                             neg_items_emb_0.norm(2).pow(2)) # L2 loss\n","\n","    pos_scores = torch.mul(users_emb_final, pos_items_emb_final)\n","    pos_scores = torch.sum(pos_scores, dim=-1) # predicted scores of positive samples\n","    neg_scores = torch.mul(users_emb_final, neg_items_emb_final)\n","    neg_scores = torch.sum(neg_scores, dim=-1) # predicted scores of negative samples\n","\n","\n","    bpr_loss = -torch.mean(torch.nn.functional.softplus(pos_scores - neg_scores))\n","\n","    loss = bpr_loss + reg_loss\n","\n","    return loss"]},{"cell_type":"markdown","id":"e7c5c914","metadata":{"id":"e7c5c914"},"source":["# Evaluation Metrics\n","\n","We evalaluate our model using the following metrics\n","\n","\\begin{equation}\n","\\text{Recall} = \\frac{TP}{TP + FP}\n","\\end{equation}\n","\n","\\begin{equation}\n","\\text{Precision} = \\frac{TP}{TP + FN}\n","\\end{equation}\n","\n","Recall@k and Precision@k is just applying only the topK recommended items and then for the overall\n","Recall@k and Precision@k, it's just averaged by the number of users\n","\n","**Dicounted Cumulative Gain (DCG)** at rank position p is defined as:\n","\n","\\begin{equation}\n","\\text{DCG}_\\text{p} = \\sum_{i = 1}^p \\frac{2^{rel_i} - 1}{\\log_2{(i + 1)}}\n","\\end{equation}\n","\n","p: a particular rank position\n","\n","$rel_i \\in \\{0, 1\\}$ : graded relevance of the result at position $i$\n","\n","**Idealised Dicounted Cumulative Gain (IDCG)**, namely the maximum possible DCG, at rank position $p$ is defined as:\n","\n","\\begin{equation}\n","\\text{IDCG}_\\text{p} = \\sum_{i = 1}^{|REL_p|} \\frac{2^{rel_i} - 1}{\\log_2{(i + 1)}}\n","\\end{equation}\n","\n","$|REL_p|$ : list of items ordered by their relevance up to position p\n","\n","**Normalized Dicounted Cumulative Gain (NDCG)** at rank position $p$ is defined as:\n","\n","\\begin{equation}\n","\\text{nDCG}_\\text{p} = \\frac{\\text{DCG}_p}{\\text{nDCG}_p}\n","\\end{equation}\n","\n","Specifically, we use the metrics recall@K, precision@K, and NDCG@K. @K indicates that these metrics are computed on the top K recommendations."]},{"cell_type":"code","execution_count":202,"id":"4a6dc66e","metadata":{"executionInfo":{"elapsed":193,"status":"ok","timestamp":1709766046971,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"4a6dc66e"},"outputs":[],"source":["def get_user_positive_items(edge_index):\n","    \"\"\"\n","    Generates dictionary of positive items for each user\n","\n","    Args:\n","        edge_index (torch.Tensor): 2 by N list of edges\n","\n","    Returns:\n","        dict: user -> list of positive items for each\n","    \"\"\"\n","\n","    # key: user_id, val: item_id list\n","    user_pos_items = {}\n","\n","    for i in range(edge_index.shape[1]):\n","        user = edge_index[0][i].item()\n","        item = edge_index[1][i].item()\n","\n","        if user not in user_pos_items:\n","            user_pos_items[user] = []\n","\n","        user_pos_items[user].append(item)\n","\n","    return user_pos_items"]},{"cell_type":"code","execution_count":203,"id":"ace2aa34","metadata":{"executionInfo":{"elapsed":218,"status":"ok","timestamp":1709766048894,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"ace2aa34"},"outputs":[],"source":["# computes recall@K and precision@K\n","def RecallPrecision_ATk(groundTruth, r, k):\n","    \"\"\"Computers recall @ k and precision @ k\n","\n","    Args:\n","        groundTruth (list[list[long]]): list of lists of item_ids. Cntaining highly rated items of each user.\n","                            In other words, this is the list of true_relevant_items for each user\n","\n","        r (list[list[boolean]]): list of lists indicating whether each top k item recommended to each user\n","                            is a top k ground truth (true relevant) item or not\n","\n","        k (int): determines the top k items to compute precision and recall on\n","\n","    Returns:\n","        tuple: recall @ k, precision @ k\n","    \"\"\"\n","\n","    # number of correctly predicted items per user\n","    # -1 here means I want to sum at the inner most dimension\n","    num_correct_pred = torch.sum(r, dim=-1)\n","\n","    # number of items liked by each user in the test set\n","    user_num_liked = torch.Tensor([len(groundTruth[i]) for i in range(len(groundTruth))])\n","\n","    recall = torch.mean(num_correct_pred / user_num_liked)\n","    precision = torch.mean(num_correct_pred) / k\n","    return recall.item(), precision.item()"]},{"cell_type":"code","execution_count":204,"id":"fa475ca5","metadata":{"executionInfo":{"elapsed":195,"status":"ok","timestamp":1709766051945,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"fa475ca5"},"outputs":[],"source":["# computes NDCG@K\n","def NDCGatK_r(groundTruth, r, k):\n","    \"\"\"Computes Normalized Discounted Cumulative Gain (NDCG) @ k\n","\n","    Args:\n","        groundTruth (list): list of lists containing highly rated items of each user\n","        r (list): list of lists indicating whether each top k item recommended to each user\n","            is a top k ground truth item or not\n","        k (int): determines the top k items to compute ndcg on\n","\n","    Returns:\n","        float: ndcg @ k\n","    \"\"\"\n","    assert len(r) == len(groundTruth)\n","\n","    test_matrix = torch.zeros((len(r), k))\n","\n","    for i, items in enumerate(groundTruth):\n","        length = min(len(items), k)\n","        test_matrix[i, :length] = 1\n","    max_r = test_matrix\n","    idcg = torch.sum(max_r * 1. / torch.log2(torch.arange(2, k + 2)), axis=1)\n","    dcg = r * (1. / torch.log2(torch.arange(2, k + 2)))\n","    dcg = torch.sum(dcg, axis=1)\n","    idcg[idcg == 0.] = 1.\n","    ndcg = dcg / idcg\n","    ndcg[torch.isnan(ndcg)] = 0.\n","    return torch.mean(ndcg).item()"]},{"cell_type":"code","execution_count":205,"id":"fe2d3e12","metadata":{"executionInfo":{"elapsed":184,"status":"ok","timestamp":1709766054385,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"fe2d3e12"},"outputs":[],"source":["# wrapper function to get evaluation metrics\n","def get_metrics(model,\n","                input_edge_index, # adj_mat based edge index\n","                input_exclude_edge_indices, # adj_mat based exclude edge index\n","                k):\n","    \"\"\"Computes the evaluation metrics: recall, precision, and ndcg @ k\n","\n","    Args:\n","        model (LighGCN): lightgcn model\n","\n","        edge_index (torch.Tensor): 2 by N list of edges for split to evaluate\n","\n","        exclude_edge_indices ([type]): 2 by N list of edges for split to discount from evaluation\n","\n","        k (int): determines the top k items to compute metrics on\n","\n","    Returns:\n","        tuple: recall @ k, precision @ k, ndcg @ k\n","    \"\"\"\n","    # get the embedding tensor at layer 0 after training\n","    user_embedding = model.users_emb.weight\n","    item_embedding = model.items_emb.weight\n","\n","\n","    # convert adj_mat based edge index to r_mat based edge index so we have have\n","    # the first list being user_ids and second list being item_ids for the edge index\n","    edge_index = convert_adj_mat_edge_index_to_r_mat_edge_index(input_edge_index)\n","\n","    # This is to exclude the edges we have seen before in our predicted interaction matrix (r_mat_rating)\n","    # E.g: for validation set, we want want to exclude all the edges in training set\n","    exclude_edge_indices = [convert_adj_mat_edge_index_to_r_mat_edge_index(exclude_edge_index) \\\n","                                      for exclude_edge_index in input_exclude_edge_indices]\n","\n","\n","\n","    # Generate predicted interaction matrix (r_mat_rating)\n","    # (num_users x 64) dot_product (num_item x 64).T\n","    r_mat_rating = torch.matmul(user_embedding, item_embedding.T)\n","\n","    # shape: num_users x num_item\n","    rating = r_mat_rating\n","\n","    for exclude_edge_index in exclude_edge_indices:\n","        # gets all the positive items for each user from the edge index\n","        # it's a dict: user -> positive item list\n","        user_pos_items = get_user_positive_items(exclude_edge_index)\n","\n","        # get coordinates of all edges to exclude\n","        exclude_users = []\n","        exclude_items = []\n","        for user, items in user_pos_items.items():\n","            # [user] * len(item) can give us [user1, user1, user1...] with len of len(item)\n","            # this makes easier to do the masking below\n","            exclude_users.extend([user] * len(items))\n","            exclude_items.extend(items)\n","\n","        # set the excluded entry in the rat_mat_rating matrix to a very small number\n","        rating[exclude_users, exclude_items] = -(1 << 10)\n","\n","    # get the top k recommended items for each user\n","    _, top_K_items = torch.topk(rating, k=k)\n","\n","    # get all unique users in evaluated split\n","    users = edge_index[0].unique()\n","\n","    # dict of user -> pos_item list\n","    test_user_pos_items = get_user_positive_items(edge_index)\n","\n","    # convert test user pos items dictionary into a list of lists\n","    test_user_pos_items_list = [test_user_pos_items[user.item()] for user in users]\n","\n","\n","    # r here is \"pred_relevant_items ∩ actually_relevant_items\" list for each user\n","    r = []\n","    for user in users:\n","        user_true_relevant_item = test_user_pos_items[user.item()]\n","        # list of Booleans to store whether or not a given item in the top_K_items for a given user\n","        # is also present in user_true_relevant_item.\n","        # this is later on used to compute n_rel_and_rec_k\n","        label = list(map(lambda x: x in user_true_relevant_item, top_K_items[user]))\n","        r.append(label)\n","\n","    r = torch.Tensor(np.array(r).astype('float'))\n","\n","    recall, precision = RecallPrecision_ATk(test_user_pos_items_list, r, k)\n","    ndcg = NDCGatK_r(test_user_pos_items_list, r, k)\n","\n","    return recall, precision, ndcg"]},{"cell_type":"code","execution_count":206,"id":"b69b4158","metadata":{"executionInfo":{"elapsed":200,"status":"ok","timestamp":1709766057936,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"b69b4158"},"outputs":[],"source":["# wrapper function to evaluate model\n","def evaluation(model,\n","               edge_index, # adj_mat based edge index\n","               exclude_edge_indices,  # adj_mat based exclude edge index\n","               k,\n","               lambda_val\n","              ):\n","    \"\"\"Evaluates model loss and metrics including recall, precision, ndcg @ k\n","\n","    Args:\n","        model (LighGCN): lightgcn model\n","        edge_index (torch.Tensor): 2 by N list of edges for split to evaluate\n","        sparse_edge_index (sparseTensor): sparse adjacency matrix for split to evaluate\n","        exclude_edge_indices ([type]): 2 by N list of edges for split to discount from evaluation\n","        k (int): determines the top k items to compute metrics on\n","        lambda_val (float): determines lambda for bpr loss\n","\n","    Returns:\n","        tuple: bpr loss, recall @ k, precision @ k, ndcg @ k\n","    \"\"\"\n","    # get embeddings\n","    users_emb_final, users_emb_0, items_emb_final, items_emb_0 = model.forward(edge_index)\n","\n","    r_mat_edge_index = convert_adj_mat_edge_index_to_r_mat_edge_index(edge_index)\n","\n","    edges = structured_negative_sampling(r_mat_edge_index, contains_neg_self_loops=False)\n","\n","    user_indices, pos_item_indices, neg_item_indices = edges[0], edges[1], edges[2]\n","\n","    users_emb_final, users_emb_0 = users_emb_final[user_indices], users_emb_0[user_indices]\n","\n","    pos_items_emb_final, pos_items_emb_0 = items_emb_final[pos_item_indices], items_emb_0[pos_item_indices]\n","\n","    neg_items_emb_final, neg_items_emb_0 = items_emb_final[neg_item_indices], items_emb_0[neg_item_indices]\n","\n","    loss = bpr_loss(users_emb_final,\n","                    users_emb_0,\n","                    pos_items_emb_final,\n","                    pos_items_emb_0,\n","                    neg_items_emb_final,\n","                    neg_items_emb_0,\n","                    lambda_val).item()\n","\n","\n","    recall, precision, ndcg = get_metrics(model,\n","                                          edge_index,\n","                                          exclude_edge_indices,\n","                                          k)\n","\n","    return loss, recall, precision, ndcg"]},{"cell_type":"code","execution_count":null,"id":"af8b0cc8","metadata":{"id":"af8b0cc8"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"316ba67b","metadata":{"id":"316ba67b"},"source":["# Training\n","\n","Your test set performance should be in line with the following (*K=20*):\n","\n","*Recall@K: 0.13, Precision@K: 0.045, NDCG@K: 0.10*"]},{"cell_type":"code","execution_count":207,"id":"c4cee1b8","metadata":{"executionInfo":{"elapsed":235,"status":"ok","timestamp":1709766061269,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"c4cee1b8"},"outputs":[],"source":["# define contants\n","ITERATIONS = 1000\n","EPOCHS = 10\n","# ITERATIONS = 500\n","BATCH_SIZE = 1024\n","LR = 1e-3\n","ITERS_PER_EVAL = 200\n","ITERS_PER_LR_DECAY = 200\n","K = 20\n","LAMBDA = 1e-6\n","# LAMBDA = 1/2"]},{"cell_type":"code","execution_count":208,"id":"1d3b2d75","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":166,"status":"ok","timestamp":1709766076868,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"1d3b2d75","outputId":"83b7c692-2e14-410f-b75c-d9ec4619975e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device cpu.\n"]}],"source":["# setup\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device {device}.\")\n","\n","\n","model = model.to(device)\n","model.train()\n","\n","optimizer = optim.Adam(model.parameters(), lr=LR)\n","scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n","\n","edge_index = edge_index.to(device)\n","\n","# # setup\n","# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# print(f\"Using device {device}.\")\n","\n","\n","# model = model.to(device)\n","# model.train()\n","\n","# optimizer = optim.Adam(model.parameters(), lr=LR)\n","# scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n","\n","# edge_index = edge_index.to(device)\n","# train_edge_index = train_edge_index.to(device)\n","# val_edge_index = val_edge_index.to(device)"]},{"cell_type":"code","execution_count":209,"id":"371bdb47","metadata":{"executionInfo":{"elapsed":185,"status":"ok","timestamp":1709766085358,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"371bdb47"},"outputs":[],"source":["def get_embs_for_bpr(model, input_edge_index):\n","    users_emb_final, users_emb_0, items_emb_final, items_emb_0 = model.forward(input_edge_index)\n","\n","\n","    edge_index_to_use = convert_adj_mat_edge_index_to_r_mat_edge_index(input_edge_index)\n","\n","    # mini batching for eval and calculate loss\n","    user_indices, pos_item_indices, neg_item_indices = sample_mini_batch(BATCH_SIZE, edge_index_to_use)\n","\n","    # This is to push tensor to device so if we are using GPU\n","    user_indices, pos_item_indices, neg_item_indices = user_indices.to(device), pos_item_indices.to(device), neg_item_indices.to(device)\n","\n","\n","    # we need layer0 embeddings and the final embeddings (computed from 0...K layer) for BPR loss computing\n","    users_emb_final, users_emb_0 = users_emb_final[user_indices], users_emb_0[user_indices]\n","    pos_items_emb_final, pos_items_emb_0 = items_emb_final[pos_item_indices], items_emb_0[pos_item_indices]\n","    neg_items_emb_final, neg_items_emb_0 = items_emb_final[neg_item_indices], items_emb_0[neg_item_indices]\n","\n","    return users_emb_final, users_emb_0, pos_items_emb_final, pos_items_emb_0, neg_items_emb_final, neg_items_emb_0"]},{"cell_type":"code","execution_count":null,"id":"fcbf4944","metadata":{"id":"fcbf4944"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":210,"id":"5b7f792e","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":942,"referenced_widgets":["f283c5d468e040ccab5aeb1b6a2c81ab","a365884dfff34aafb5d114ab82210126","cc8bd4a3424b48c39e058dbdfc4648f9","f2d588a674474de49c6925b5077cdaa3","502ddb6f7e234ca9bd102b1bd3c11302","b710185b34c5412cac5e5d463d3b0299","a6c4d7dc44cc4deabcfc57aee998a803","8f1060669c6f46a8a4265f60df3ad959","6e2ea6c006bf4352b482e8f04cb4fddf","4ed2d6de157d46ea95e58e64737e0d73","0ffbab77ce5c4e1a8bd48f0b7b5a847a"]},"executionInfo":{"elapsed":345965,"status":"ok","timestamp":1709766434174,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"5b7f792e","outputId":"cf31e0f6-0d42-43a2-ef2e-c311fffbe21e"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a89d064000b447bb93e9406446b3ad9b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Training\n","for iter in tqdm(range(ITERATIONS)):\n","    # forward propagation\n","    users_emb_final, users_emb_0,  pos_items_emb_final, pos_items_emb_0, neg_items_emb_final, neg_items_emb_0 \\\n","                = get_embs_for_bpr(model, edge_index)\n","\n","    # loss computation\n","    train_loss = bpr_loss(users_emb_final,\n","                          users_emb_0,\n","                          pos_items_emb_final,\n","                          pos_items_emb_0,\n","                          neg_items_emb_final,\n","                          neg_items_emb_0,\n","                          LAMBDA)\n","    \n","    optimizer.zero_grad()\n","    train_loss.backward()\n","    optimizer.step()\n","\n","    if iter % ITERS_PER_LR_DECAY == 0 and iter != 0:\n","        scheduler.step()\n","\n","# # training loop\n","# train_losses = []\n","# val_losses = []\n","# val_recall_at_ks = []\n","\n","# for iter in tqdm(range(ITERATIONS)):\n","#     # forward propagation\n","#     users_emb_final, users_emb_0,  pos_items_emb_final, pos_items_emb_0, neg_items_emb_final, neg_items_emb_0 \\\n","#                 = get_embs_for_bpr(model, train_edge_index)\n","\n","#     # loss computation\n","#     train_loss = bpr_loss(users_emb_final,\n","#                           users_emb_0,\n","#                           pos_items_emb_final,\n","#                           pos_items_emb_0,\n","#                           neg_items_emb_final,\n","#                           neg_items_emb_0,\n","#                           LAMBDA)\n","\n","#     optimizer.zero_grad()\n","#     train_loss.backward()\n","#     optimizer.step()\n","\n","#     # validation set\n","#     if iter % ITERS_PER_EVAL == 0:\n","#         model.eval()\n","\n","#         with torch.no_grad():\n","#             val_loss, recall, precision, ndcg = evaluation(model,\n","#                                                            val_edge_index,\n","#                                                            [train_edge_index],\n","#                                                            K,\n","#                                                            LAMBDA\n","#                                                           )\n","\n","#             print(f\"[Iteration {iter}/{ITERATIONS}] train_loss: {round(train_loss.item(), 5)}, val_loss: {round(val_loss, 5)}, val_recall@{K}: {round(recall, 5)}, val_precision@{K}: {round(precision, 5)}, val_ndcg@{K}: {round(ndcg, 5)}\")\n","\n","#             train_losses.append(train_loss.item())\n","#             val_losses.append(val_loss)\n","#             val_recall_at_ks.append(round(recall, 5))\n","#         model.train()\n","\n","#     if iter % ITERS_PER_LR_DECAY == 0 and iter != 0:\n","#         scheduler.step()"]},{"cell_type":"code","execution_count":217,"id":"423573ef","metadata":{"id":"423573ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([200, 988])\n","max: tensor(468.9757)\n","min tensor(-95.5455)\n","mean tensor(48.3430)\n","torch.Size([200, 10])\n"]}],"source":["import pickle\n","\n","model.eval()\n","\n","with torch.no_grad():\n","    users_emb_final, users_emb_0, items_emb_final, items_emb_0 = model.forward(edge_index)\n","    r_mat_rating = torch.matmul(users_emb_final, items_emb_final.T)\n","\n","rating = r_mat_rating\n","\n","print(r_mat_rating.shape)\n","print('max:', torch.max(r_mat_rating))\n","print('min', torch.min(r_mat_rating))\n","print('mean', torch.mean(r_mat_rating))\n","\n","np.save('lp_mat.npy', r_mat_rating)\n","\n","with open('lp_dict.pkl', 'wb') as f:\n","    pickle.dump(movieId_to_idx, f)\n","\n","# get the top k recommended items for each user\n","_, top_K_items = torch.topk(rating, k=10)\n","print(top_K_items.shape)"]},{"cell_type":"code","execution_count":219,"id":"d394d542","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([345.6321, 128.6997,  89.4843, 207.2257, 139.3368, 132.5201, 338.8462,\n","        255.2886, 313.4667, 155.8008, 250.5537, 268.7529, 222.8406, 256.8444,\n","        272.0758,  66.3725, 111.0183,  44.9296, 144.6240, 119.6576])\n"]}],"source":["print(torch.max)"]},{"cell_type":"markdown","id":"dabdd722","metadata":{"id":"dabdd722"},"source":["# Plot"]},{"cell_type":"code","execution_count":213,"id":"6e32fd3b","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"elapsed":967,"status":"ok","timestamp":1709766797962,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"6e32fd3b","outputId":"e17141ea-92ba-4db2-a7af-ed587d470d46"},"outputs":[],"source":["# iters = [iter * ITERS_PER_EVAL for iter in range(len(train_losses))]\n","# plt.plot(iters, train_losses, label='train')\n","# plt.plot(iters, val_losses, label='validation')\n","# plt.xlabel('iteration')\n","# plt.ylabel('loss')\n","# plt.title('training and validation loss curves')\n","# plt.legend()\n","# plt.show()"]},{"cell_type":"code","execution_count":214,"id":"203b99c0","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"elapsed":755,"status":"ok","timestamp":1709766802335,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"203b99c0","outputId":"6286237d-7f35-4b09-def7-e3fd5f8eb1ef"},"outputs":[],"source":["# f2 = plt.figure()\n","# plt.plot(iters, val_recall_at_ks, label='recall_at_k')\n","# plt.xlabel('iteration')\n","# plt.ylabel('recall_at_k')\n","# plt.title('recall_at_k curves')\n","# plt.show()"]},{"cell_type":"code","execution_count":215,"id":"c9c96a3f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":459,"status":"ok","timestamp":1709766806005,"user":{"displayName":"Isaac Orais","userId":"15687764132659357223"},"user_tz":480},"id":"c9c96a3f","outputId":"29ff40b9-274b-419f-840b-1212e471e3cb"},"outputs":[],"source":["# # evaluate on test set\n","# model.eval()\n","# test_edge_index = test_edge_index.to(device)\n","\n","# test_loss, test_recall, test_precision, test_ndcg = evaluation(model,\n","#                                                                test_edge_index,\n","#                                                                [train_edge_index, val_edge_index],\n","#                                                                K,\n","#                                                                LAMBDA\n","#                                                               )\n","\n","# print(f\"[test_loss: {round(test_loss, 5)}, test_recall@{K}: {round(test_recall, 5)}, test_precision@{K}: {round(test_precision, 5)}, test_ndcg@{K}: {round(test_ndcg, 5)}\")"]},{"cell_type":"code","execution_count":null,"id":"fb7489fd","metadata":{"id":"fb7489fd"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1sdJORFmKNGULZQdqgF7EtoqO1hELbemm","timestamp":1709722499795}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0ffbab77ce5c4e1a8bd48f0b7b5a847a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ed2d6de157d46ea95e58e64737e0d73":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"502ddb6f7e234ca9bd102b1bd3c11302":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e2ea6c006bf4352b482e8f04cb4fddf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8f1060669c6f46a8a4265f60df3ad959":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a365884dfff34aafb5d114ab82210126":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b710185b34c5412cac5e5d463d3b0299","placeholder":"​","style":"IPY_MODEL_a6c4d7dc44cc4deabcfc57aee998a803","value":"100%"}},"a6c4d7dc44cc4deabcfc57aee998a803":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b710185b34c5412cac5e5d463d3b0299":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc8bd4a3424b48c39e058dbdfc4648f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f1060669c6f46a8a4265f60df3ad959","max":10000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6e2ea6c006bf4352b482e8f04cb4fddf","value":10000}},"f283c5d468e040ccab5aeb1b6a2c81ab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a365884dfff34aafb5d114ab82210126","IPY_MODEL_cc8bd4a3424b48c39e058dbdfc4648f9","IPY_MODEL_f2d588a674474de49c6925b5077cdaa3"],"layout":"IPY_MODEL_502ddb6f7e234ca9bd102b1bd3c11302"}},"f2d588a674474de49c6925b5077cdaa3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ed2d6de157d46ea95e58e64737e0d73","placeholder":"​","style":"IPY_MODEL_0ffbab77ce5c4e1a8bd48f0b7b5a847a","value":" 10000/10000 [05:45&lt;00:00, 27.46it/s]"}}}}},"nbformat":4,"nbformat_minor":5}
